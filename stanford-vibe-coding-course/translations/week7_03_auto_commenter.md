# Week 7-3: AI-Assisted Assessment of Coding Practices in Modern Code Review
# 现代代码审查中的 AI 辅助编码实践评估 (AutoCommenter)

> **Original Link**: [https://arxiv.org/abs/2405.13565](https://arxiv.org/abs/2405.13565)
> **Title**: AI-Assisted Assessment of Coding Practices in Modern Code Review
> **Author**: Google Research & Core Systems (Manushree Gupta et al.)
> **Week**: 7
> **Reading Time**: 25 min
> **Priority**: High
> **Translation Date**: 2026-02-03

---

## 摘要 (Abstract)
现代代码审查是一个过程，在此过程中，代码作者所做的增量代码贡献在提交到版本控制系统之前，会由一个或多个同行进行审查。现代代码审查的一个重要元素是验证代码贡献是否遵循最佳实践。虽然其中一些最佳实践可以自动验证，但验证其他的通常留给人类审查者。本文报告了 AutoCommenter 的开发、部署和评估，这是一个由大语言模型（LLM）支持的系统，可以自动学习和执行编码最佳实践。我们为四种编程语言（C++、Java、Python 和 Go）实现了 AutoCommenter，并在一个大型工业环境中评估了其性能和采用情况。我们的评估表明，用于学习和执行编码最佳实践的端到端系统是可行的，并对开发人员的工作流程产生积极影响。此外，本文还报告了将此类系统部署给数万名开发人员所面临的挑战以及相应的经验教训。

## 1. 引言 (Introduction)
现代代码审查已经从开源和工业设置中自然发展而来。通过同行评审标准，包括编码最佳实践，已经出现了一套通用的标准。许多公司、项目甚至编程语言都以“风格指南（style guides）”的形式正式定义了它们，通常涵盖以下方面：
- **格式化（Formatting）**：行限制、空格和缩进的使用、圆括号和方括号的位置等；
- **命名（Naming）**：大小写、简洁性、描述性等；
- **文档（Documentation）**：文件级、函数级和其他注释的预期位置和内容；
- **语言特性（Language features）**：在不同（代码）上下文中使用特定的语言特性；
- **代码惯用语（Code idioms）**：使用代码惯用语来提高代码的清晰度、模块化和可维护性。

开发人员通常对现代代码审查流程表示高度满意。主要好处之一是为不熟悉代码库、特定语言特性或常见代码惯用语的代码作者提供学习经验。在审查期间，专家开发人员就最佳实践教育代码作者。

静态分析工具（如 Linter）可以自动验证代码是否遵循某些最佳实践（例如，格式化规则），有些工具甚至可以自动修复违规行为。然而，细微的指导方针或有例外的指导方针很难完全自动验证（例如，命名约定和遗留代码中的合理偏差），有些指导方针根本无法通过精确的规则捕获（例如，代码注释的清晰度和特异性），依赖于人类判断和集体开发人员知识。因此，通常期望人类审查者检查代码更改是否违反最佳实践。

代码审查流程的最大成本是所需的时间，特别是专家开发人员的时间。即使进行了显著的自动化，并且保持流程尽可能轻量级，开发人员每天也可以轻松地在此任务上花费数小时。

机器学习的最新进展，特别是大语言模型（LLM）的能力，表明 LLM 适用于代码审查自动化。然而，围绕大规模部署端到端系统的软件工程挑战仍然未被探索。同样，对此类系统在整体功效和用户接受度方面的外部评估也是缺失的。

## 2. 背景 (Background)

### 2.1 代码审查流程
Google 的代码审查流程是成熟的、基于变更的且有工具辅助的。对代码库的每一项更改都必须至少由另一位开发人员审查。每天，对代码库的数万次更改都会经过审查流程，数万名开发人员作为代码作者和审查者参与其中。

审查中最昂贵的部分是代码作者和审查者“照管（shepherding）”变更所花费的时间（从初始编码，到解决审查者评论和确保所有自动分析通过，最后将变更合并到代码库）。虽然流程已通过在审查前分析代码的自动化系统进行了优化（特别是无需人工干预的自动代码格式化），但代码审查每年仍消耗数千个开发人员年。因此，即使是个位数的百分比节省也能转化为巨大的业务影响。

### 2.2 最佳实践
最佳实践是指被认为优越的特定编程语言用法，最佳实践文档描述了应如何应用它以及它带来的好处。最佳实践 URL 指的是最佳实践文档或其中的特定部分，最佳实践违规指的是不符合最佳实践但可以更改以符合最佳实践的特定代码片段。

Google 的中央代码仓库包含多种语言的代码，其中 C++、Java、Python 和 Go 各超过 1 亿行。对于 15 种不同的语言，所有开发人员都可以随时获得正式的风格指南。许多语言还有额外的语言入门读物、核心库文档和每周提示风格的时事通讯。虽然这些材料不像风格指南那样严格执行，但在代码审查中经常被引用。

一种名为“可读性（readability）”的正式机制在十多年前被引入，以确保一致地遵循最佳实践。特定语言的专门风格专家，称为“可读性导师（readability mentors）”，指导缺乏经验的开发人员精通该语言。可读性导师通常用几句话总结最佳实践，并在评论末尾包含一个 URL 供变更作者参考。

可读性流程也有一些缺点。对于作者来说，由于额外的审查轮次，它增加了开发时间。对于可读性导师来说，这可能变成一项单调且耗时的任务。它需要掌握数百个不断发展的最佳实践，包括识别和弃用过时的规则，并在代码审查系统中通过相关链接记录它们。

## 3. 方法 (Approach)

### 3.1 模型和任务定义
自动化最佳实践分析需要一个能够表示源代码、精确定位违规位置并识别被违规的最佳实践的模型。我们使用基于 T5 的传统 Transformer 方法，通过 T5X 针对文本到文本的转换。

最佳实践分析是多任务大序列模型中的一项任务。除了 T5 的标准预训练任务、跨度去噪（预测被掩盖的 token）之外，用于训练此模型的其他任务包括代码审查评论解决、下一次编辑预测、变量重命名和构建错误修复。训练语料库包含超过 30 亿个示例，其中最佳实践分析数据集贡献了约 80 万个示例。

对于最佳实践分析，模型的输入是一个任务提示（task prompt）和源代码，目标（target）是一个源代码位置和一个最佳实践违规的 URL。任务提示被格式化为固定文本代码注释，使用编程语言适当的注释风格。它用自然语言描述任务，并在源代码之前，源代码是一个文件的直接文本表示。如果输入超过模型上下文窗口，它将被截断。位置是源代码中的字节偏移量，URL 引用了被违规的最佳实践。领域特定语言（DSL）定义了目标格式，如果没有违规，则为“空”目标这一特殊情况。除了目标之外，模型还会输出一个范围从 0 到 1 的置信度分数。

**示例（Go 语言）：**
*Input*:
```go
// [*] Task: Check language best practices.
// Package addition provides Add
package addition

// Return a sum
func Add(value1, value2 int) int {
    return value1 + value2
}
```
*Target*:
```text
INSERT 153 COMMENT https://go.dev/doc/comment#func
```
目标给出了位置（字节偏移量 153 对应于 Add 函数的开始）和一个 go.dev URL，指向该函数注释违反的 Go 语言风格指南的确切部分（在这种情况下，通常的做法是以函数名开始注释）。

### 3.2 模型训练
训练管道包括三个部分：
1.  **大规模预处理（Large-scale preprocessing）**：从真实代码审查数据中创建训练示例。此步骤识别相关的代码评论——包含指向最佳实践文档 URL 的人工评论。对于每条评论，它收集相应的源代码和相关元数据。
2.  **数据集策展（Dataset curation）**：这是一个按需处理步骤，将每个相关代码评论转换为标准的 TensorFlow Example 数据结构。
3.  **训练和微调（Training and fine-tuning）**：策展后的示例直接用于模型训练和评估。我们使用 TPU 队列上的 T5X 框架。

### 3.3 模型选择
为了准确衡量实时环境中的潜在评论量，我们在历史代码审查集上使用特定模型检查点和阈值评估 AutoCommenter。预测的评论不会追溯发布到代码审查系统中，而是记录在数据库中进行分析。这使我们能够估计预期的发布频率。

### 3.4 推理基础设施
AutoCommenter 的核心是一个中央最佳实践分析服务。该服务接受一个或多个源文件作为输入。对于每个文件，它构建模型输入，查询模型，并执行一系列过滤步骤以抑制低质量预测。

### 3.5 IDE 和代码审查集成
开发人员以两种方式与 AutoCommenter 交互：
1.  **IDE 插件**：AutoCommenter 的评论显示为带有蓝色波浪下划线的诊断信息。悬停在下划线代码上会显示完整评论，其中包含最佳实践的简明摘要和指向相关文档的可点击链接。
2.  **代码审查系统**：AutoCommenter 在每次更新（即每个新的代码审查快照）后运行，如果检测到任何违规，会自动发布评论。自动工具产生的评论在视觉上类似于人类产生的评论，但背景颜色不同。

评论包含“赞（thumbs up）”、“踩（thumbs down）”按钮，以及对审查者可见的“请修复（Please fix）”按钮。

## 4. 部署 (Deployment)

我们于 2022 年 7 月至 2023 年 10 月期间向 Google 的所有开发人员部署了 AutoCommenter：
- **直到 2022 年 7 月**：团队内部试用（本文作者）。
- **2022 年 7 月**：早期采用者（约 3000 名志愿者）。
- **2023 年 7 月**：A/B 实验（约一半的开发人员）。
- **2023 年 10 月起**：全面可用（所有开发人员）。

我们使用迭代改进方法持续评估和改进 AutoCommenter 的性能。

### 4.1 选择阈值和解码策略
**阈值**：在初始部署期间，我们从高置信度阈值 t=0.98 开始。我们观察到，即使在阈值以下，约 80% 的预测仍然是正确的（即假阴性率很高）。此外，Python 的预测显示出显着不同的置信度分布。每个 URL 的阈值（基于验证数据集上的内在评估计算）比单一的每种语言阈值更有效。

**解码**：评估显示，AutoCommenter 在 6% 的所有更改文件中检测到违规。然而，80% 的评论会发布在作者未修改的代码行上。AutoCommenter 过滤掉未更改代码行上的评论，将比率降低到 1.3%。为了提高这一比率，我们尝试了 Beam Search（生成 n=4 个潜在响应），这将发布频率增加了两倍，达到 3.9%。它还产生了更高的 URL 多样性。我们决定在代码审查系统中使用 Beam Search，在 IDE 中使用 Greedy Search（为了低延迟）。

### 4.2 抑制过时的最佳实践
在向早期采用者发布后，我们注意到针对特定 URL 的大量问题报告（例如关于 Python 导入的规则已更改）。由于我们的训练数据包含 2022 年之前的数据，它包含不再适用的最佳实践评论。我们选择使用条件过滤（在源代码上匹配正则表达式）来抑制特定最佳实践预测，这比重新训练模型更快速、更灵活。

### 4.3 独立评论评级
为了理解为什么“有用率（useful ratio）”停滞在 54% 左右，我们在 2023 年 4 月进行了一项独立的人类评级研究。发现了一些无用评论的模式：
- **多个主题或复杂主题**：URL 指向描述多个指导方针的章节，作者难以理解具体指代哪个。
- **高质量摘要的重要性**：摘要有时未能充分解释引用的指导方针与评论/代码的相关性。
- **主观和潜在争议的主题**：例如关于在库代码中避免使用 flag 的规则，并没有被模型学好其中的细微差别。
- **正确但低价值的评论**：例如代码注释末尾缺少句号。虽然技术上正确，但要求作者修复可能提供净负价值。

基于这些见解，我们抑制了 17 个不可操作的 URL，并手动更新了所有频繁发布 URL 的摘要。这些更改使有用率提高到了 74%。

### 4.4 A/B 实验
2023 年 7 月的 A/B 实验显示，代码审查的总持续时间、开发人员主动花费的时间或作者与审查者之间的迭代次数没有任何统计学上的显着变化。然而，我们确实检测到**编码速度略有提高**。我们推测，减少切换到文档的上下文切换会导致这种积极影响。基于这些结果，我们得出结论没有副作用，并于 2023 年 10 月向所有开发人员部署。

## 5. 评估 (Evaluation)

除了开发人员满意度（有用率）之外，我们还评估了三个方面：

### 5.1 评论解决 (Comment Resolution)
我们通过离线分析估计后续代码更改解决评论的比率。分析 6000 个快照对显示，在 50% 的情况下，评论在提交的快照中不再存在。手动检查发现，在 80% 的情况下，作者所做的更改直接解决了评论描述的问题。因此，我们估计**评论解决率约为 40%**，这明显大于带有明确积极反馈的评论比例。

### 5.2 AutoCommenter vs. 人类评论
AutoCommenter 产生的评论所使用的 URL 集合覆盖了 **68%** 的历史人类评论引用的最佳实践 URL。这是一个很好的结果，表明 AutoCommenter 并没有专注于审查者很少引用的晦涩最佳实践。
然而，URL 多样性仍然相对较低。前 85 个 URL 占 AutoCommenter 所有评论的 90%。

### 5.3 AutoCommenter vs. Lint 工具
为了了解 AutoCommenter 是否提供了超出 Linter 的价值，专家分析了前 50 个最常预测的 URL。结果显示，对于 **33/50 (66%)** 的最佳实践，违规检测超出了传统静态分析的范围。这意味着 AutoCommenter 确实在补充现有的工具，而不是重复它们。

## 6. 经验教训 (Lessons learned)
- **补充传统分析**：AutoCommenter 的 LLM 方法生成了人类审查者经常引用的 68% 的最佳实践评论。其中许多超出了传统静态分析的范围。
- **内在评估 vs. 现实世界性能**：虽然内在评估显示模型有希望，但我们的外在评估和系统改进对于成功部署至关重要。
- **监控用户接受度至关重要**：通过持续监控和分析现实世界的反馈，我们能够检测到负面用户体验并确定补救措施（如抑制机制），从而将用户接受度提高到 80% 以上，而无需在功效上做出重大牺牲。

## 8. 结论 (Conclusion)
评估结果表明，开发一个能力远超传统工具的端到端系统，同时实现高度的最终用户接受度是可行的。AutoCommenter 标志着向部署复杂的代码审查助手和自动化代码审查迈出了充满希望的第一步。未来，利用上下文窗口更大的最先进模型将开启新功能和现有功能的显著改进。
