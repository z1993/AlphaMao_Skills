---
原文链接: https://stytch.com/blog/model-context-protocol-introduction/
原文标题: Model Context Protocol (MCP): A comprehensive introduction for developers
所属周次: Week 2
阅读时间: 15min
优先级: ⭐必读
翻译日期: 2026-02-03
---

# 模型上下文协议 (MCP): 开发者的全面介绍

**作者**: Reed McGinley-Stempel
**日期**: 2025年3月28日

## 执行摘要
模型上下文协议 (MCP) 是一个开放标准，它连接了 AI 模型与外部数据和服务，允许大型语言模型 (LLM) 以一致、安全的方式进行结构化的 API 调用。本文将介绍 MCP，解释它为何对连接 AI 系统有价值，将其与 ChatGPT 插件和手动 API 集成等现有方法进行比较，并深入探讨其最近对基于 OAuth 的身份验证的支持。我们还将探索一些代码，看看 MCP 的实际应用。

MCP 充当 AI 工具和外部服务之间的通用适配器，消除了为每个工具或 API 编写自定义集成代码的需要。就像 USB-C 简化了不同设备之间的连接一样，MCP 提供了一种统一的方法，供 AI 模型调用外部函数、检索数据或使用预定义的提示。其核心在于，MCP 提供了一个接口，可以连接到许多不同的系统。

## 什么是模型上下文协议 (MCP)?
MCP本质上是 AI 应用程序与外部工具或数据源之间的通用适配器。它定义了一个[通用协议](https://www.anthropic.com/news/model-context-protocol)（建立在 JSON-RPC 2.0 之上），让 AI 助手以结构化的方式调用外部服务的函数、获取数据或使用预定义的提示。不再需要每个 LLM 应用程序为每个 API 或数据库编写自定义代码，[MCP 为所有交互提供了一种标准化的“语言”](https://medium.com/data-and-beyond/the-model-context-protocol-mcp-the-ultimate-guide-c40539e2a8e7)。

MCP 层使 AI 应用程序能够安全地访问和与外部数据源和工具交互。它充当大型语言模型 (LLM) 与各种数据库、应用程序或 API 之间的桥梁，促进无缝集成和功能，而无需大量的自定义编码。

MCP 使用客户端-服务器架构来实现这一点。AI 驱动的应用程序（例如聊天机器人、IDE 助手或代理）充当主机并运行 MCP 客户端组件，而每个外部集成都作为一个 MCP 服务器运行。服务器通过 MCP 协议公开功能（如函数、数据资源或提示模板），客户端连接到它以使用这些功能。这种分离意味着 AI 模型不直接与 API 对话；相反，它通过 MCP 客户端/服务器握手，该握手结构化了交换过程。

## 为什么 MCP 有价值
在传统的设置中，将 AI 模型连接到外部数据或操作是繁琐且临时的。开发人员通常必须为他们希望模型使用的每个 API 或数据库编写一次性集成，处理每个集成的不同身份验证、数据格式和错误处理。MCP 通过标准化这些交互改变了游戏规则。主要好处包括：

- **快速工具集成**: 使用 MCP，你可以插入新功能，而无需从头开始编写代码。如果存在用于 Google Drive 或 SQL 数据库的 MCP 服务器，任何兼容 MCP 的 AI 应用程序都可以连接到它并立即获得该能力。这对自动化来说是一个巨大的胜利——AI 代理可以根据需要获取文档、查询数据库或调用 API，只需添加适当的服务器即可。就像拥有一个现成的“插件”库，它们都说同一种语言。
- **自主代理**: MCP 赋予了更自主的 AI 行为。代理不限于其内置知识；它们可以主动检索信息或在多步工作流中执行操作。例如，复杂的代理可能使用 MCP 从 CRM 收集数据，然后通过通信工具发送电子邮件，然后在数据库中记录一条记录——所有这些都在一个无缝链中完成。
- **减少摩擦和设置**: 由于 MCP 充当通用接口，开发人员避免了维护单独集成的碎片化。一旦应用程序支持 MCP，它就可以通过单一机制连接到任意数量的服务。这大大减少了每次你想让 AI 使用新 API 时所需的手动设置。
- **一致性和互操作性**: MCP 强制执行跨工具的一致请求/响应格式。这意味着你的 AI 应用程序不必处理服务 A 的 HTTP 响应、服务 B 的 XML 响应等。模型的输出（函数调用）和工具结果都以[统一的 JSON 结构](https://dev.to/fotiecodes/function-calling-vs-model-context-protocol-mcp-what-you-need-to-know-4nbo)传递。这种一致性使得调试和扩展变得更加容易。
- **双向上下文**: 与简单的 API 调用不同，MCP 支持维护模型和工具之间的上下文和持续对话。MCP 服务器除了工具之外，还可以提供提示（针对特定任务的预定义提示模板）和资源（如文档的数据上下文）。这意味着 AI 不仅可以“调用 API”，还可以摄取参考数据或遵循由服务器指导的复杂工作流。

## MCP 架构 – 一目了然
MCP 遵循清晰的客户端-服务器架构：
- **MCP 客户端**: 嵌入在 AI 应用程序中（聊天机器人、IDE 助手、自动化代理）。
- **MCP 服务器**: 公开外部功能，如函数（工具）、资源（数据）和提示（模板）。

所有交互都通过标准化的 JSON-RPC 消息进行，保持安全、结构化的交换。

**示例 JSON-RPC 请求**:
例如，要列出可用工具，MCP 客户端发送如下请求：
```json
{ "jsonrpc": "2.0", "id": 1, "method": "tools/list", "params": {} }
```

服务器将回复一个结构化的 JSON，列出工具（每个工具都有名称、描述和输入模式）。
```json
{ "jsonrpc": "2.0", "id": 1, "result": [ { "name": "get_weather", "description": "Retrieves weather data.", "schema": { "location": "string" } } ] }
```

后来，当 LLM 决定使用工具时，客户端会调用它。MCP 服务器执行该函数并在结构化 JSON 响应中返回结果。主机应用程序调节此过程：它将 LLM 的意图（通常通过模型的函数调用输出）转换为 MCP 请求，然后将服务器的结构化结果传回给 LLM。

## 与其他方法的比较

| 特性 | MCP | 自定义集成 | ChatGPT 插件 |
| :--- | :--- | :--- | :--- |
| **集成速度** | ✅ 快速，即插即用 | ❌ 慢，自定义代码 | ⚠️ 中等，专有 |
| **身份验证** | ✅ OAuth 标准 | ❌ 手动 API 密钥 | ⚠️ 插件特定的 OAuth |
| **交互类型** | ✅ 连续且上下文丰富 | ❌ 临时交互 | ❌ 单次交互 |
| **开放标准** | ✅ 是 | ❌ 否 | ❌ 否 |

- **自定义集成**: 传统方法容易导致系统脆弱，且难以扩展。MCP 将这些交互中心化和标准化。
- **ChatGPT 插件**: 这是标准化工具使用的早期步骤，但它是专有的且大多是一次性调用。MCP 是开放和通用的，支持丰富的双向交互。

## 早期限制与解决方案（身份验证）
为了解决最初的身份验证限制并增强安全连接，MCP 采用了 **OAuth 2.0**，这是一个广泛认可的强大身份验证标准。
1. **动态客户端注册 (DCR)**: 允许客户端自动向 OAuth 服务器注册，无需手动设置。
2. **自动端点发现**: 利用标准化的元数据 URL 自动发现 OAuth 端点。
3. **安全授权和令牌管理**: 客户端安全地获取根据用户权限和访问范围定制的 OAuth 令牌。
4. **可扩展的多用户支持**: OAuth 2.0 的设计天生支持多个并发用户和服务。

## 结论
MCP 为增强 LLM 带来了一种可扩展的、即插即用的方法。它让 AI 系统能够安全地利用它们需要的“实时”数据和操作，而无需每个开发人员重新发明轮子。

---

## 关键术语

| 英文 | 中文 | 说明 |
|------|------|------|
| Model Context Protocol (MCP) | 模型上下文协议 | 连接 AI 模型与外部工具的开放标准 |
| MCP Client | MCP 客户端 | 运行在 AI 应用（如 Claude Desktop, IDE）中的组件 |
| MCP Server | MCP 服务器 | 提供具体的工具、资源或提示的服务端 |
| JSON-RPC | JSON-RPC | 一种无状态、轻量级的远程过程调用 (RPC) 协议 |
| Resources | 资源 | MCP 中的一种原语，代表类似于文件的数据上下文 |
| Prompts | 提示 | MCP 中的一种原语，代表预定义的提示模板 |
| Tools | 工具 | MCP 中的一种原语，代表可执行的函数 |
| OAuth 2.0 | OAuth 2.0 | 一种行业标准的授权协议 |
