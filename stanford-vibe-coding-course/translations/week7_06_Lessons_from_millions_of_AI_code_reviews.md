---

原文链接: https://www.youtube.com/watch?v=TswQeKftnaw

原文标题: AI powered entomology: Lessons from millions of AI code reviews

讲师: Tomas Reimers (Co-founder, Graphite)

所属周次: Week 7-6

阅读时间: 约 15-20 分钟 (全文逐字级翻译)

优先级: 📖选读

翻译日期: 2026-02-02

---



\# AI 驱动的昆虫学：从百万次 AI 代码审查中学到的教训



\*\*\[00:00:00] 开场白\*\*



非常感谢大家来到这里，感谢大家参加这次会议。我的名字是 Tomas，我是 \*\*Graphite\*\* 的联合创始人之一。今天我来和大家谈谈 \*\*AI 驱动的昆虫学 (AI Powered Entomology)\*\*。



对于那些不知道这个词的人来说，“Entomology”是昆虫学，也就是对虫子（Bugs）的研究。这非常贴近我们要做的核心事情，也是我们产品功能的一部分。



为了让大家了解背景，Graphite 构建了一个名为 \*\*Diamond\*\* 的产品。Diamond 是一个 AI 驱动的代码审查器。它的工作原理是你把它连接到你的 GitHub 仓库，它就会自动去寻找代码中的 Bug。



这个项目大约在一年前开始。我们要解决的根本问题是：我们注意到 AI 编写的代码数量在不断增加（比如使用 Copilot），但与此同时，Bug 的数量也在增加。经过深思熟虑，我们认为这两者实际上是相辅相成的。鉴于技术的进步，我们需要找到一种更好的方法来解决这些 Bug。



---



\## 1. AI 能找到 Bug 吗？



\*\*\[00:01:03] 初步尝试\*\*



我们首先求助的对象就是 AI 本身。我们开始问这样一个问题：也许是 AI 制造了这些 Bug，但它能不能也帮我们找到 Bug？它能在这个过程中帮助我们吗？



我们开始尝试做一些非常简单的事情，比如直接问 Claude：“嘿，这是一个 \*\*Pull Request (PR)\*\*，你能在这个 PR 上找到 Bug 吗？”



早期的结果其实让我们印象非常深刻。



\* \*\*案例 1：数据库崩溃\*\*

&nbsp;   这实际上是从我们本周的代码库中提取的一个真实例子。AI 发现，在某些特定情况下，我们返回了一个未实例化的数据库 \*\*ORM (对象关系映射)\*\* 类。如果这段代码上线，会导致我们的服务器崩溃。AI 敏锐地捕捉到了这一点。



\* \*\*案例 2：前端数学错误\*\*

&nbsp;   这是本周推特上流传的一个例子，我们的机器人发现，在某些情况下，围绕 `border-radius`（边框圆角）进行的数学运算会导致除以负数，进而导致前端渲染崩溃。



所以，回答“AI 能否找到 Bug”这个问题：事实证明，答案是肯定的，AI 确实可以找到 Bug。



\*\*\[00:01:46] 挫折与现实\*\*



演讲结束……开个玩笑。



如果你自己尝试过这个（用 LLM 审查代码），你可能也有过非常、非常令人沮丧的经历。我们也看到过类似的情况。除了找到 Bug，它还会说：

\* “你应该更新这段代码以执行它的功能。”（这是废话）

\* “CSS 不是这样工作的。”（实际上 CSS 就是这样工作的，AI 产生了幻觉）

\* 或者我最喜欢的：“你应该把这段代码恢复成原来的样子，因为那是它原来的样子。”（完全无视了修改的意图）



收到这些评论让我们失去了很多信心。但我们开始反思：我们既看到了一些非常好的结果，也看到了一些非常糟糕的结果。这让我们意识到，也许 Bug 不止一种类型？也许 LLM 能找到的东西也不止一种类型？



我们决定对其进行分类。



---



\## 2. 重新分类：不仅是“对与错”



\*\*\[00:02:25] LLM 的模仿本能\*\*



归根结底，\*\*LLM (大型语言模型)\*\* 最终会试图模仿你要求它做的事情。如果你问它：“嘿，在这个 PR 上通常会留下什么样的代码审查评论？”它会试图模仿人类，留下所有的评论——既包括它能力范围内的（能做对的），也包括它能力范围之外的（会做错的）。



我们开始尝试对这些评论进行分类。但我们发现，即使我们过滤掉了明显的错误，LLM 还是会开始留下这样的评论：

\* “你应该添加一条注释来描述这个类的作用。”

\* “你应该将此逻辑提取到一个函数中。”

\* “你应该确保这段代码有测试。”



虽然这些评论对开发人员来说在\*\*技术上是正确的\*\*（Technically Correct），但它们真的非常令人沮丧。你不想让一个机器人来教你这些琐碎的事情。



\*\*\[00:03:02] 关键洞察：人类的接受度\*\*



我认为这实际上是我们构建这个项目中最具洞察力的时刻之一。



当我们与设计团队坐下来，开始实际逐条检查过去的 Bug 评论时——既包括我们的机器人留下的，也包括人类在我们自己的代码库中留下的——开发人员的意见表现出惊人的一致性：

\* “是的，如果 LLM 留下这个评论，我可以接受。”

\* “不，如果 LLM 留下那个评论，我会觉得很烦，我不接受。”

\* “是的，这个可以。”



而我们的设计师实际上对此感到困惑，他们会问：“但这看起来和那个评论很像啊，为什么这个行，那个不行？”



我认为在开发人员的脑海中发生的事情是：如果你去阅读这种类型的评论（比如代码风格建议），当它来自 \*\*人类同事\*\* 时，你可能会觉得这是一种指导或讨论；但当它来自 \*\*LLM\*\* 时，你会觉得它迂腐、令人沮丧、烦人。



\*\*\[00:03:43] 第二个轴：人类想要什么？\*\*



因此，当我们开始更多地思考 Bug 的分类时，我们引入了 \*\*第二个轴\*\*。所以现在我们有了两个维度：

1\.  \*\*能力轴\*\*：LLM 能捕捉的 vs. LLM 不能捕捉的（准确性）。

2\.  \*\*意愿轴\*\*：人类希望从 LLM 那里收到的 vs. 人类不希望从 LLM 那里收到的（合意性）。



为了验证这一点，我们做了大量工作：我们从我们自己的代码库和开源代码库中提取了 \*\*10,000 条评论\*\*。我们将它们提供给各种 LLM，并要求它们对其进行分类。我们不仅做了一次，而是做了很多次。然后我们对这些评论进行了总结和聚类分析。



---



\## 3. 四象限分析：AI 代码审查的甜蜜点



\*\*\[00:04:21] 图表解析\*\*







最终我们得到了这张图表（PPT 展示四象限图）。它表明在现实世界的代码库中，评论分为四类：



\*\*(1) 右上象限：AI 能捕捉 且 人类想要接收 (The Sweet Spot)\*\*

这是我们要追求的目标。

\* \*\*逻辑 Bug\*\*：代码逻辑不一致，导致行为不符合预期。

\* \*\*意外提交的代码 (Accidentally committed code)\*\*：这种情况出现的频率比你预期的要高（比如提交了调试用的 `print` 语句）。

\* \*\*性能和安全问题\*\*：例如 SQL 注入风险或低效的循环。

\* \*\*文档不一致\*\*：代码说的是一回事，文档说的是另一回事，不清楚哪个是对的。

\* \*\*风格变更\*\*：比如“在这个代码库中我们遵循这种模式”。



\*\*(2) 右下象限：人类想要接收 但 AI 做不到 (Limitations)\*\*

\* \*\*部落知识 (Tribal Knowledge)\*\*：你在 PR 中经常看到的一类评论是“嘿，我们以前是这样做的，但因为某某历史原因，我们不再这样做了”。

\* 这种知识并不存在于纸面文档上，它存在于你的资深开发人员的脑子里。这很棒，但对于 AI 来说，要通过“读心术”来获取这些信息几乎是不可能的。AI 没有这种历史上下文。



\*\*(3) 左上象限：AI 能捕捉 但 人类不想接收 (The Annoying Zone)\*\*

这是最容易让人误入的陷阱。

\* \*\*代码整洁度 (Code Cleanliness)\*\* 和 \*\*最佳实践\*\*。

\* 例子：“注释这个函数”、“添加测试”、“将此类型提取到不同的文件中”、“将此逻辑提取到函数中”。

\* \*\*为什么不想要？\*\* 虽然这些建议在\*\*技术上总是正确的\*\*，但作为一个人类，你在应用这些规则时会有一个内部的“晴雨表”：

&nbsp;   \* 你会判断：“在这个代码库中，这部分逻辑特别棘手，我觉得以后有人会被绊倒，所以我们应该现在把它提取出来。”（这是有价值的）

&nbsp;   \* VS “在这个代码库中，这样写其实没问题，不用过度设计。”

\* \*\*AI 的问题\*\*：机器人没有这种判断力，它几乎总是会留下这种评论。它会变得像一个唠叨的学究。虽然它没说错，但它不受欢迎。



\*\*(4) 左下象限：AI 做不到 且 人类也不想要 (Hallucinations)\*\*

\* 纯粹的幻觉或错误的建议。这是我们要极力避免的。



\*\*\[00:06:02] 结论\*\*



我想说的一点是，随着我们向 AI 提供更多的上下文（Context），右上象限（舒适区）似乎会变大。但就目前而言，考虑到我们现有的上下文、代码库、历史记录和规则，这就是我们所面临的现状。



---



\## 4. 如何衡量成功？(Measuring Success)



\*\*\[00:06:35] 持续的挑战\*\*



如果你使用过 LLM 开发产品，你就知道：离线测试和第一遍测试对于初始分类很棒，但更难的问题是：\*\*在生产环境中，你如何知道你是持续正确的？\*\*



故事是这样的：我们基于上述发现，更新了我们的 \*\*提示词 (Prompts)\*\*，明确指示 LLM \*\*只\*\* 做那些在其能力范围内且人类想要接收的事情（右上象限）。轶事证据表明，人们开始更喜欢它了，抱怨变少了。



但当我们试图升级模型（比如用 \*\*Claude 3 Opus\*\* 替代 \*\*Sonnet\*\*），或者增加上下文窗口时，我们如何确保我们仍然停留在“右上象限”？我们如何知道我们没有意外地滑入“左上象限”（烦人的建议）或“左下象限”（幻觉）？



\*\*\[00:07:26] 指标 1：显性反馈 (Upvotes/Downvotes)\*\*



首先，我们实施了一个简单的机制。

\* 我们在产品中添加了 \*\*点赞 (Upvotes)\*\* 和 \*\*点踩 (Downvotes)\*\* 按钮。

\* 这基本上告诉了我们 LLM 何时在产生幻觉（左下象限）。

\* 当我们开始看到点踩率激增时，我们就知道：“好吧，我们可能试图将这个东西扩展到它的能力之外了，我们需要将其调低。”



目前的成果：我们的点踩率不到 \*\*4%\*\*，我们对此感觉相当不错。这解决了“准确性”的问题。



\*\*\[00:08:27] 指标 2：隐性反馈 (Fix Rate)\*\*



第二个指标要难得多。如何衡量“人类想要接收”？如何区分“有用的建议”和“虽然正确但烦人的建议”？



我们思考的是：\*\*留评论的根本目的是什么？\*\*

为什么你要在代码审查中留下评论？最终是为了让某人根据评论 \*\*更新代码\*\* 以反映这一点。



所以我们的问题变成了：我们能衡量这个吗？我们能衡量 \*\*有多少百分比的评论实际上导致了它们所描述的代码更改\*\* 吗？



\*\*\[00:08:54] 发现：人类的基准线\*\*



我们开始在开源仓库以及 Graphite 有权访问的各种仓库上分析这个问题。

我认为我们发现的最令人着迷的数据点之一是：

\*\*在人类与人类的代码审查中，只有大约 50% 的评论会导致代码变更。\*\*



这让我们很惊讶，但也给了我们一个基准。我们开始问：我们能让 LLM 至少达到这个水平吗？因为如果我们能让它达到这个水平，它至少是在以人类的“保真度”水平留下评论。



\*\*\[00:09:10] 为什么不是 100%？\*\*



你可能坐在观众席上想：“为什么不是 100% 的评论都会导致行动？难道我们说废话吗？”

我想对这个数字做一个说明：我说的是 \*\*在该 PR 本身内导致行动\*\*。

\* \*\*向前修复 (Fix Forward)\*\*：很多评论是这样的，作者会说：“嘿，我听到你的意见了，但这太紧急了，我会在后续的 PR 中修复这个问题。”

\* \*\*教育性评论\*\*：很多评论是：“嘿，提醒一下，将来如果你这样做，也许你可以用另一种方式做。”这些不需要当时就修改代码。

\* \*\*风格偏好\*\*：还有一些纯粹是偏好问题：“我会这样写，但你那样写也没错。”在健康的代码审查文化中，这种分歧空间是存在的。



\*\*\[00:09:43] AI 的表现\*\*



所以我们开始以这个为目标。

随着时间的推移，通过不断的提示工程优化和模型调整，我们确实做到了。

截至今年 3 月，Diamond 的修复率达到了 \*\*52%\*\*。



这意味着，当 AI 提出建议时，开发人员采纳并修改代码的概率，实际上已经略微超过了人类同事的平均水平。



\*\*\[00:10:00] 总结\*\*



我认为我们更广泛的论点是：通过 LLM 来寻找 Bug、进行代码审查实际上是行之有效的，但这需要精细的分类和对人类心理的理解。



如果你想在生产环境中尝试这些发现，\*\*Diamond\*\* 是我们提供的产品。我们在那边有一个展位，欢迎来交流。谢谢大家。



---



\## 关键术语表



| 英文 | 中文 | 说明 |

|------|------|------|

| \*\*AI Powered Entomology\*\* | AI 驱动的昆虫学 | 演讲者创造的双关语，指利用 AI 技术来寻找和研究软件 Bug（Entomology 原意为昆虫学）。 |

| \*\*Pull Request (PR)\*\* | 拉取请求 | 代码协作中的核心机制，开发者请求将代码更改合并到主分支，通常伴随着代码审查。 |

| \*\*ORM (Object-Relational Mapping)\*\* | 对象关系映射 | 一种将数据库数据转换为面向对象编程语言中对象的技术。 |

| \*\*Hallucination\*\* | 幻觉 | AI 模型生成看起来自信但实际上错误、不存在或无意义的信息。 |

| \*\*Tribal Knowledge\*\* | 部落知识 | 未记录在文档中，只存在于特定团队成员（如资深员工）头脑中的隐性知识或历史背景。 |

| \*\*Fix Rate / Action Rate\*\* | 修复率/行动率 | 代码审查评论导致实际代码更改的百分比。这是衡量评论有效性的关键指标。 |

| \*\*Fix Forward\*\* | 向前修复 | 在代码审查中，同意在未来的更新或后续 PR 中解决当前指出的问题，而不是阻碍当前的合并。 |

| \*\*Pedantic\*\* | 迂腐的/学究气的 | 指过分关注微小的细节或规则，而忽视了整体的实用性或上下文。 |

