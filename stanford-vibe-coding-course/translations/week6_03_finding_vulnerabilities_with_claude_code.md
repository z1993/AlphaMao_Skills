---
原文链接: https://semgrep.dev/blog/2025/finding-vulnerabilities-in-modern-web-apps-using-claude-code-and-openai-codex/
原文标题: Finding vulnerabilities in modern web apps using Claude Code and OpenAI Codex
所属周次: Week 6
阅读时间: 25min
优先级: ⭐实战
翻译日期: 2026-02-03
---

# 使用 Claude Code 和 OpenAI Codex 发现现代 Web 应用中的漏洞

**来源**: Semgrep Blog

我们对 AI 编码代理发现漏洞能力的深入研究揭示了令人惊讶的优势、严重的弱点以及一致性方面的严重问题。

---

#### 太长不看 (TL;DR)
我们评估了 AI 编码代理在发现真实代码漏洞方面的有效性。
*   **如何做**: 我们让 Anthropic 的 Claude Code (v1.0.32, Sonnet 3.5) 和 OpenAI Codex (v0.2.0, o4-mini) 在 11 个流行的大型开源 Python Web 应用程序中寻找漏洞。它们总共产生了超过 400 个安全发现，我们的安全研究团队对此进行了审查。
*   **AI 编码代理发现了真正的漏洞**: Claude Code 发现了 46 个漏洞（**14% 真阳性率 TPR**, 86% 假阳性率 FPR），Codex 报告了 21 个漏洞（**18% TPR**, 82% FPR）。其中约 20 个是高严重性漏洞。
*   **AI 理解上下文，但在数据流上卡壳**: Claude Code 最擅长发现**不安全的直接对象引用 (IDOR)** 漏洞，具有 22% (13/59) 的 TPR，但在跨多个文件和函数执行污点跟踪时很吃力，SQL 注入的 TPR 为 5% (2/38)，XSS 为 16% (12/74)。OpenAI Codex 在报告任何正确的 IDOR 方面都很挣扎 (0% TPR)，在 SQL 注入 (0%) 和 XSS (0%) 方面也表现很差，但在路径遍历问题上出人意料地报告了比 Claude Code 更多的正确问题 (47% TPR)。
*   **非确定性是一个真正的斗争**: 在同一个代码库上多次运行完全相同的 Prompt 往往会产生截然不同的结果。在一个应用程序中，三次相同的运行分别产生了 3、6 和 11 个不同的发现。

---

## 关键要点 (Key take-aways)
*   具有相对简单的以安全为中心的 Prompt 的 AI 编码代理**已经可以**在实际应用程序中找到真正的漏洞。
*   **但是**: 取决于漏洞类别，结果可能非常嘈杂（高假阳性率），尤其是在传统的注入式漏洞类别（如 SQL 注入、XSS 和 SSRF）上。
*   详细的评估和基准测试对于理解一种方法在发现漏洞方面的有效性至关重要（这一直是正确的，但对于像 LLM 这样的非确定性工具来说更是如此），并且要知道你的改进是否朝着正确的方向发展。
*   这是我们关于使用和增强 AI 以发现漏洞系列的第一篇文章！

---

## 介绍 (Introduction)
在 Semgrep，我们以应用安全 (AppSec) 为生。我们在产品中实施 AI 已经很长时间了，不断研究传统确定性分析与现代 AI 上下文能力的最佳结合。
本研究是该持续使命的一部分。我们开始了一项深入的、公开的探索，以回答每个人心中的问题：**LLM 在源代码中发现漏洞到底有多有效？**

### 关于 AI 漏洞搜寻的开放研究问题
为了指导我们的调查，我们将“LLM 擅长发现 Bug 吗？”这个广泛的问题分解为更具体、可衡量的问题。
*   LLM 漏洞发现的误报率和漏报率是多少？它是否因编程语言、框架、代码库大小或漏洞类别而异？对代码编写方式的敏感度如何？
*   误报和漏报的常见原因是什么？
*   分析的确定性如何？每次都能得到相同的结果吗？

对于**注入漏洞**，我们特别想知道：
*   LLM 在跟踪从源（例如，用户提供的值）到汇（例如，使用原始 SQL 查询的方法）的用户输入方面有多有效？
*   它能跨函数和文件跟踪数据吗？
*   它能推理出清理函数和其他安全控制吗？
*   它能推理出第三方开源依赖项中的函数吗？

---

## 通常 SAST 基准测试的问题：缺乏现实性
在深入研究我们的发现之前，让我们先谈谈如果不衡量 AI 性能。目前的大部分研究/声明都依赖于基准测试，虽然有价值，但并不能完全捕捉现实世界代码的复杂性。
*   **具有已知漏洞的应用**: 这些基准测试仅使用具有已知漏洞的开源应用（如 WebGoat, JuiceShop）。问题是，当前的 LLM 可能已经摄入了这些存储库的代码以及互联网上的大量公开分析，给了它们先验知识并歪曲了结果。代码通常不切实际，充满了暗示漏洞位置的注释和变量名。
*   **XBOW 基准**: XBOW 为自动渗透测试工具提供了基准，但不适合静态分析：代码通常过于做作和简单，不代表真实的应用程序。
*   **CVE 记忆偏差**: 由于 LLM 是在海量公共数据上训练的，包括发现和修复 CVE 的代码库。这这就产生了根本的数据污染问题。AI 可能不是通过新颖的分析检测到漏洞，而只是识别出它在训练期间记忆的模式。

---

## 实验：AI vs. 现实世界应用代码 (The Experiment)

### 我们发现了什么 (What we Found)
*   **Claude Code 和 Codex 今天都有用**: 它们发现了真正的安全漏洞，但非常嘈杂。
*   **许多 IDOR Bug 起初看起来是正确的，Claude Code 建议了可信的修复**: LLM 不仅能找到这些 Bug，还能建议可信的修复方案，比如基于代码中现有模式注入新的权限检查。
*   **Claude Code 作为好的安全护栏工具？**: 许多发现虽然技术上是误报，但仍然是很好的“护栏”建议。例如，模型经常建议参数化一个实际上已经安全的 SQL 查询。虽然不是漏洞，但这强化了代码。但要注意，我们发现了一些案例，特别是在客户端 JavaScript 中，AI 试图修复它认为的 DOM 操作问题，实际上却破坏了代码（双重转义）。
*   **XSS 和 SQL 注入——难以将组件拼凑在一起**: 模型很难将数据从服务器端框架追踪到客户端组件，或者通过复杂的应用层。它经常被静态定义的数据搞糊涂，或者未能识别服务器端清理。
*   **重复 Prompt 有助于获得更多结果**: LLM 的概率性质意味着以稍微不同的方式问同一个问题可能会产生新的发现。在同一个代码库上多次重复相同的 Prompt 给出了不同的答案。
*   **OpenAI Codex 有时未能报告有效的 SARIF**: 66 个预期报告中有 9 个不是有效的 SARIF。Claude Code 没有这个问题。

---

## 同样的代码，同样的 AI，每次都不同的 Bug：AI 编码代理的非确定性问题
为了探索非确定性在实践中如何表现，我们选择了其中三个应用程序，并用**相同的 Prompt 多次运行**，针对同一个安全问题：IDOR。出现了一个模式：**AI 的发现在我们要运行的每一次测试中都是不同的。**

在漏洞检测的背景下，这是一个主要问题。
首先，作为一名安全工程师，理想情况下我们希望对我们的代码是否针对重要漏洞类别进行了扫描有更强的保证，而不是“希望模型这次搜索得很彻底”。
其次，间歇性地检测漏洞会导致安全工具或漏洞管理系统中的不一致和噪音。

### 我们观察到的具体例子：
*   **PY-APP-007**: 第一次运行发现了一个“缺失搜索授权”的漏洞，而后两次没有。第二次运行标记了该运行独有的问题。
*   **PY-APP-002**: 这里的变异性最为显著。已识别漏洞的数量从第一次运行的 3 个跳到第二次的 6 个，再到 11 个。每次运行都提出了仅部分重叠的不同发现。

### 为什么会这样？
我们认为关键因素是**上下文腐烂 (Context Rot)** 和 **压缩 (Compaction)**。
当 AI 代理被指派分析整个代码库时，它处理着海量信息。上下文腐烂导致无法从其自身上下文中准确检索。为了管理这一点，LLM 使用某种形式的有损压缩（有时称为压缩），这意味着像函数名、路径等更精细的推理细节可能会在总结过程中丢失。
就像试图总结一本长篇小说一样，你会捕捉到主要情节点，但难免会遗漏一些微妙之处。同样，AI 可能会失去对特定架构模式或微妙数据流的跟踪。

---

## Claude Code 新的 `/security-review` 命令效果如何？
Anthropic 发布了一个名为 `/security-review` 的新命令。它旨在在 Pull Request 上运行，检查更改的文件并提出问题以识别特定的安全问题。
当在**整个代码库**上运行此命令时，我们发现识别出的安全问题相当**有限**。很多时候，它无法找到我们在提示 Claude Code 一次搜索一种特定类型的安全问题时得到的那种安全问题。
我们在三个应用上运行了此命令，它在所有应用中只发现了一个 XSS，这与我们在整体实验中得到的结果大相径庭。

## 结论 (Conclusion)
在构建时发现并修复重要问题。AI 编码代理在发现漏洞方面显示出了前景，但目前的局限性（噪音大、非确定性、上下文丢失）意味着它们还不能完全替代传统的确定性分析工具。然而，作为一个辅助的“第二双眼睛”，通过正确的 Prompt 和人工审查，它们可以发现传统工具可能遗漏的复杂逻辑漏洞。
