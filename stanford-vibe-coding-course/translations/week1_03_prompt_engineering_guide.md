---
原文链接: https://www.promptingguide.ai/techniques
原文标题: Prompting Techniques | Prompt Engineering Guide
所属周次: Week 1
阅读时间: 30min
优先级: ⭐必读
翻译日期: 2026-02-03
---

# 提示工程技术 (Prompting Techniques)

提示工程 (Prompt Engineering) 有助于有效地设计和改进提示，以便在大型语言模型 (LLM) 的不同任务中获得更好的结果。

虽然之前的基本示例很有趣，但在本节中，我们将介绍更高级的提示工程技术，这些技术使我们能够实现更复杂的任务，并提高 LLM 的可靠性和性能。

---

## 1. 零样本提示 (Zero-Shot Prompting)
**原文链接**: [Zero-Shot Prompting](https://www.promptingguide.ai/techniques/zeroshot)

如今的大型语言模型（LLM），如 GPT-3.5 Turbo、GPT-4 和 Claude 3，都经过微调以遵循指令，并在大量数据上进行了训练。大规模训练使这些模型能够以“零样本”的方式执行某些任务。零样本提示意味着用于与模型交互的提示不包含示例或演示。零样本提示直接指示模型执行任务，无需任何额外的示例来引导它。

**示例**
```
Prompt:
将文本分类为中性、消极或积极。
文本：我觉得这个假期还行。
情感：

Output:
中性
```

请注意，在上面的提示中，我们没有为模型提供任何文本及其分类的示例，LLM 已经理解了“情感”——这就是零样本能力在起作用。

指令微调（Instruction tuning）已被证明可以改善零样本学习。指令微调本质上是在通过指令描述的数据集上微调模型的概念。此外，已采用 RLHF（来自人类反馈的强化学习）来扩展指令微调，其中模型对齐以更好地适应人类偏好。最新的发展为 ChatGPT 等模型提供了动力。

当零样本不起作用时，建议在提示中提供演示或示例，这导致了少样本提示。

---

## 2. 少样本提示 (Few-Shot Prompting)
**原文链接**: [Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot)

虽然大型语言模型展示了非凡的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不足。少样本提示可以用作一种技术来启用上下文学习（in-context learning），我们在提示中提供演示以引导模型获得更好的性能。演示作为后续示例的条件，我们希望模型在这些后续示例中生成响应。

**示例**
```
Prompt:
"whatpu" 是一种原产于坦桑尼亚的小型毛茸茸的动物。使用单词 whatpu 的句子示例是：我们在非洲旅行时看到了这些非常可爱的 whatpus。
做 "farduddle" 意味着快速跳上跳下。使用单词 farduddle 的句子示例是：

Output:
当我们赢得比赛时，我们都开始 farduddle 以庆祝。
```

我们可以观察到，模型通过仅提供一个示例（即 1-shot），就以某种方式学会了如何执行任务。对于更困难的任务，我们可以尝试增加演示（例如 3-shot、5-shot、10-shot 等）。

---

## 3. 思维链提示 (Chain-of-Thought Prompting)
**原文链接**: [Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot)

思维链 (CoT) 提示通过中间推理步骤启用复杂的推理能力。你可以将其与少样本提示结合使用，以便在需要推理才能响应的更复杂任务上获得更好的结果。

**示例**
```
Prompt:
这组中的奇数加起来是偶数：4, 8, 9, 15, 12, 2, 1。
A：将所有奇数相加 (9, 15, 1) 得到 25。答案是 False。

这组中的奇数加起来是偶数：17, 10, 19, 4, 8, 12, 24。
A：将所有奇数相加 (17, 19) 得到 36。答案是 True。

... (更多示例) ...

这组中的奇数加起来是偶数：15, 32, 5, 13, 82, 7, 1。
A：

Output:
将所有奇数相加 (15, 5, 13, 7, 1) 得到 41。答案是 False。
```

我们可以看到，当我们提供推理解释步骤时，结果是完美的。实际上，我们只需提供一个示例似乎就足够了。

---

## 4. 检索增强生成 (Retrieval Augmented Generation, RAG)
**原文链接**: [Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag)

通用语言模型可以进行微调以实现几种常见任务，如情感分析和命名实体识别。这些任务通常不需要额外的背景知识。
对于更复杂和知识密集的任务，可以构建一个基于语言模型的系统，该系统访问外部知识源来完成任务。这实现了更多的事实一致性，提高了生成响应的可靠性，并有助于缓解“幻觉”问题。

Meta AI 研究人员介绍了一种称为检索增强生成 (RAG) 的方法来解决此类知识密集型任务。RAG 将信息检索组件与文本生成器模型相结合。RAG 可以进行微调，其内部知识可以以有效的方式进行修改，而无需重新训练整个模型。

RAG 接受输入并根据源（例如维基百科）检索一组相关/支持文档。这些文档作为上下文与原始输入提示连接，并馈送到生成最终输出的文本生成器。这使得 RAG 适应事实可能随时间演变的情况。这非常有用，因为 LLM 的参数知识是静态的。RAG 允许语言模型绕过重新训练，从而能够访问最新信息以通过基于检索的生成来生成可靠的输出。

---

## 5. ReAct 提示
**原文链接**: [ReAct Prompting](https://www.promptingguide.ai/techniques/react)

Yao 等人 (2022) 引入了一个名为 ReAct 的框架，其中 LLM 用于以交错的方式生成推理轨迹和特定于任务的操作。
生成推理轨迹允许模型归纳、跟踪和更新行动计划，甚至处理异常。行动步骤允许与外部源（如知识库或环境）进行接口和收集信息。

ReAct 框架允许 LLM 与外部工具交互以检索导致更可靠和事实响应的额外信息。
结果表明，ReAct 在语言和决策任务上可以优于几个最先进的基线。ReAct 还提高了 LLM 的人类可解释性和可信度。总体而言，作者发现最佳方法是结合使用 ReAct 和思维链 (CoT)，这允许在推理过程中同时使用内部知识和获得的外部信息。

**ReAct 是如何工作的？**
第一步是从训练集中选择案例（例如 HotPotQA）并组成 ReAct 格式的轨迹。这些在提示中用作少样本示例。轨迹由多个思考-行动-观察步骤组成。自由形式的思考用于实现不同的任务，如分解问题、提取信息、执行常识/算术推理、指导搜索公式化和合成最终答案。

---

## 6. 元提示 (Meta Prompting)
**原文链接**: [Meta Prompting](https://www.promptingguide.ai/techniques/meta-prompting)

元提示是一种高级提示技术，侧重于任务和问题的结构及语法方面，而不是其具体的内容细节。元提示的目标是构建一种更抽象、更结构化的与大型语言模型 (LLM) 交互的方式，强调信息的某种形式和模式，而不是传统的内容中心方法。

**主要特征**
1.  **面向结构**: 优先考虑问题和解决方案的格式和模式，而不是具体内容。
2.  **关注语法**: 使用语法作为预期响应或解决方案的指导模板。
3.  **抽象示例**: 采用抽象示例作为框架，说明问题和解决方案的结构，而不关注具体细节。
4.  **通用性**: 适用于各种领域，能够为各种问题提供结构化响应。
5.  **分类方法**: 借鉴类型理论，强调提示中组件的分类和逻辑排列。

---

## 7. 自洽性 (Self-Consistency)
**原文链接**: [Self-Consistency](https://www.promptingguide.ai/techniques/consistency)

也许目前最先进的提示工程技术之一是自洽性。由 Wang 等人 (2022) 提出，自洽性旨在“取代思维链提示中使用的朴素贪婪解码”。其想法是通过少样本 CoT 对多个不同的推理路径进行采样，并使用生成的答案来选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理的任务上的性能。

**示例**
```
Prompt:
当我 6 岁时，我妹妹是我年龄的一半。现在我 70 岁了，我妹妹多大？

Output:
35
```
输出是错误的！我们如何通过自洽性来改善这一点？让我们试一试。我们将生成多个推理路径，然后取最一致的答案（在这个例子中，即 67 岁）。

---

## 8. 生成知识提示 (Generate Knowledge Prompting)
**原文链接**: [Generated Knowledge Prompting](https://www.promptingguide.ai/techniques/knowledge)

LLM 不断得到改进，一种流行的技术包括整合知识或信息以帮助模型做出更准确预测的能力。
使用类似的想法，模型是否也可以用于在进行预测之前生成知识？这正是 Liu 等人 (2022) 在论文中尝试的做法——生成用作提示的一部分的知识。特别是，这对于诸如常识推理之类的任务有多大帮助？

这种方法首先要求模型针对给定的输入生成一些相关的“知识”，然后将这些知识作为额外的上下文提供给模型，以帮助其回答最终的问题。这有助于填补模型在某些具体领域知识上的空白或纠正其推理偏差。

---

## 9. 提示链 (Prompt Chaining)
**原文链接**: [Prompt Chaining](https://www.promptingguide.ai/techniques/prompt_chaining)

为了提高 LLM 的可靠性和性能，重要的提示工程技术之一是将任务分解为其子任务。一旦确定了这些子任务，就会向 LLM 提示一个子任务，然后将其响应作为另一个提示的输入。这就是所谓的提示链，其中任务被拆分为子任务，目的是创建一个提示操作链。

提示链对于完成复杂的任务非常有用，如果使用非常详细的提示，LLM 可能难以解决这些任务。在提示链中，链提示在生成的响应达到最终所需状态之前对其执行转换或附加处理。
除了实现更好的性能外，提示链有助于提高 LLM 应用程序的透明度，增加可控性和可靠性。这意味着你可以更容易地调试模型响应的问题，并分析和改进需要改进的不同阶段的性能。

---

## 10. 思维树 (Tree of Thoughts, ToT)
**原文链接**: [Tree of Thoughts](https://www.promptingguide.ai/techniques/tot)

对于需要探索或战略前瞻的复杂任务，传统或简单的提示技术往往表现不足。Yao 等人 (2023) 和 Long (2023) 最近提出了思维树 (ToT)，这是一个概括了思维链提示的框架，并鼓励探索作为使用语言模型解决一般问题的中间步骤的思维。

ToT 维护一棵思维树，其中思维代表连贯的语言序列，作为解决问题的中间步骤。这种方法使 LM 能够通过慎重的推理过程自我评估解决问题的进度。LM 生成和评估思维的能力随后与搜索算法（例如，广度优先搜索和深度优先搜索）相结合，以实现具有前瞻和回溯的思维的系统探索。

---

## 11. 自动推理和工具使用 (ART)
**原文链接**: [Automatic Reasoning and Tool-use](https://www.promptingguide.ai/techniques/art)

以交错方式结合 CoT 提示和工具已被证明是解决 LLM 许多任务的一种强大而稳健的方法。这些方法通常需要手工制作特定于任务的演示，并未仔细编写模型生成与工具使用的交错脚本。Paranjape 等人 (2023) 提出了一个新框架，该框架使用冻结的 LLM 自动生成中间推理步骤作为程序。

ART 的工作原理如下：
- 给定一个新任务，它从任务库中选择多步推理和工具使用的演示。
- 在测试时，每当调用外部工具时，它都会暂停生成，并在恢复生成之前集成其输出。
ART 鼓励模型从演示中归纳，以零样本的方式分解新任务并在适当的地方使用工具。此外，ART 是可扩展的，因为它还允许人类通过简单地更新任务和工具库来修复推理步骤中的错误或添加新工具。

---

## 12. 自动提示工程师 (APE)
**原文链接**: [Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)

Zhou 等人 (2022) 提出了自动提示工程师 (APE)，这是一个用于自动指令生成和选择的框架。指令生成问题被构建为自然语言合成问题，使用 LLM 生成和搜索候选解决方案被作为一个黑盒优化问题来解决。

第一步涉及一个大型语言模型（作为推理模型），该模型被给予输出演示以生成任务的指令候选项。这些候选解决方案将指导搜索过程。使用目标模型执行指令，然后根据计算的评估分数选择最合适的指令。
APE 发现了一个比人工设计的“让我们一步一步思考”提示更好的零样本 CoT 提示：“让我们一步一步地解决这个问题，以确保我们有正确的答案。”（Let's work this out in a step by step way to be sure we have the right answer.）

---

## 13. 主动提示 (Active-Prompt)
**原文链接**: [Active-Prompt](https://www.promptingguide.ai/techniques/activeprompt)

思维链 (CoT) 方法依赖于一组固定的人工注释示例。这种方法的问题在于，这些示例可能不是不同任务的最有效示例。为了解决这个问题，Diao 等人 (2023) 最近提出了一种名为 Active-Prompt 的新提示方法，以使 LLM 适应不同的特定于任务的示例提示（用人工设计的 CoT 推理进行注释）。

该方法的第一步是用或不用几个 CoT 示例查询 LLM。为一组训练问题生成 k 个可能的答案。基于 k 个答案计算不确定性度量（使用分歧）。选择最不确定的问题由人类进行注释。然后使用新的注释示例来推断每个问题。

---

## 14. 定向刺激提示 (Directional Stimulus Prompting)
**原文链接**: [Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)

Li 等人 (2023) 提出了一种新的提示技术，以更好地指导 LLM 生成所需的摘要。
训练一个可调优的策略 LM 来生成刺激/提示。看到更多使用 RL 来优化 LLM。
策略 LM 可以很小，并经过优化以生成指导黑盒冻结 LLM 的提示。

---

## 15. 程序辅助语言模型 (PAL)
**原文链接**: [Program-Aided Language Models](https://www.promptingguide.ai/techniques/pal)

Gao 等人 (2022) 提出了一种方法，使用 LLM 阅读自然语言问题并将程序生成为中间推理步骤。这被称为程序辅助语言模型 (PAL)，它与思维链提示的不同之处在于，它不是使用自由形式的文本来获得解决方案，而是将解决方案步骤卸载到编程运行时，例如 Python 解释器。

---

## 16. Reflexion
**原文链接**: [Reflexion](https://www.promptingguide.ai/techniques/reflexion)

Reflexion 是一个通过语言反馈来强化基于语言的代理的框架。根据 Shinn 等人 (2023) 的说法，“Reflexion 是一种‘口头’强化的新范式，它将策略参数化为代理的记忆编码与 LLM 参数的选择配对。”

在高层次上，Reflexion 将来自环境的反馈（自由形式的语言或标量）转换为语言反馈，也称为自我反思，这作为上下文提供给下一个事件中的 LLM 代理。这有助于代理快速有效地从以前的错误中吸取教训，从而在许多高级任务上提高性能。

Reflexion 扩展了 ReAct 框架，引入了自我评估、自我反思和记忆组件。

---

## 17. 多模态思维链 (Multimodal CoT)
**原文链接**: [Multimodal CoT](https://www.promptingguide.ai/techniques/multimodalcot)

Zhang 等人 (2023) 最近提出了一种多模态思维链提示方法。传统的 CoT 侧重于语言模态。相比之下，Multimodal CoT 将文本和视觉通过一个两阶段框架结合起来。第一步涉及基于多模态信息的原理生成。紧随其后的是第二阶段，答案推理，它利用信息丰富的生成原理。
多模态 CoT 模型 (1B) 在 ScienceQA 基准测试中优于 GPT-3.5。

---

## 18. 图提示 (GraphPrompts)
**原文链接**: [Graph Prompting](https://www.promptingguide.ai/techniques/graph)

Liu 等人 (2023) 介绍了 GraphPrompt，这是一种用于图的新提示框架，旨在提高下游任务的性能。

---

## 关键术语

| 英文 | 中文 | 说明 |
|------|------|------|
| Zero-shot | 零样本 | 无需示例直接推理 |
| Few-shot | 少样本 | 提供少量示例辅助推理 |
| Chain-of-Thought (CoT) | 思维链 | 逐步推理的过程 |
| Retrieval Augmented Generation (RAG) | 检索增强生成 | 结合外部知识库的生成方式 |
| Reasoning | 推理 | 逻辑推导过程 |
| Tool-use | 工具使用 | LLM 调用外部工具（如计算器、API）的能力 |
| Hallucination | 幻觉 | 模型生成看似合理但错误的信息 |
| Instruction Tuning | 指令微调 | 在指令数据集上微调模型 |
| In-context Learning | 上下文学习 | 通过提示中的示例学习任务模式 |
| Self-Consistency | 自洽性 | 对多条推理路径采样并取一致结果 |
| Meta Prompting | 元提示 | 关注任务结构的抽象提示方法 |
| Active-Prompt | 主动提示 | 主动选择困难样本进行人工标注 |
