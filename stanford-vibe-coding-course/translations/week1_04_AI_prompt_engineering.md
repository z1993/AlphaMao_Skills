---
原文链接: https://www.youtube.com/watch?v=T9aRN5JkmL8
原文标题: AI prompt engineering: A deep dive
讲师: Anthropic Team (Alex, David, Amanda, Zach)
所属周次: Week 1-4
阅读时间: 约 60分钟
优先级: 📖选读 (但强烈推荐)
翻译日期: 2026-02-02

---

# AI 提示工程深度解析 (Part 1)

## 0. 开场介绍

**[00:00:00] Alex Albert (DevRel):**
大家好，欢迎来到这次圆桌会议。今天我们将深入探讨 **提示工程（Prompt Engineering）**。我们汇集了来自不同领域的专家，希望从研究者、消费者和企业用户的角度，全方位地审视这一领域。我们的目标是弄清楚：提示工程到底是什么？它的核心要素有哪些？

我想先请大家做一下自我介绍。我是 Alex，Anthropic 的开发者关系负责人，也曾是一名提示工程师。

**[00:00:36] David Hershey:**
我是 David，我在 Anthropic 负责客户工作。这实际上意味着涵盖了从模型定制（Model Customization）、微调（Fine-tuning），到帮助人们构建围绕模型的整个系统架构的所有事务。

**[00:00:50] Amanda Askell:**
我是 Amanda，我是微调（Fine-tuning）团队的一员。我们团队主要负责让 Claude 变得诚实、乐于助人和无害。我们负责所有的微调数据以及与之相关的很多工作。

**[00:01:03] Zach Whitten:**
我是 Zach，我是一名提示工程师。我的主要工作是尝试提升所谓的“环境提示水平（Ambient Prompt Level）”。我负责构建像提示生成器（Prompt Generator）这样的工具，编写教育材料，并帮助那些正在编写提示的人做得更好。

---

## 1. 什么是提示工程？(Defining Prompt Engineering)

**[00:02:02] Alex:**
太棒了。让我们从一个宽泛的问题开始：**什么是提示工程？** 为什么它被称为“工程”？我觉得很多人并不真正理解这意味着什么，所以我很想听听你们的定义。Zach，不如你先开始？

**[00:02:18] Zach:**
对我来说，提示工程就是 **试图让模型做事情**。这听起来很简单，但其核心在于挖掘模型的最大潜力，让它完成原本可能无法完成的任务。这很大程度上是关于 **清晰的沟通（Clear Communication）**。

与模型交谈，在很多方面和与人交谈非常相似。你需要理解模型的“心理学”，知道它是如何处理信息的。

**[00:02:50] Alex:**
那为什么叫“工程”呢？为什么不是“提示写作”或“提示指引”？

**[00:02:58] Zach:**
我认为“工程”的部分来自于 **试错（Trial and Error）** 的过程。与人类不同，你可以对模型按下一个巨大的“重置”按钮，让它回到原点（Square Zero）。这意味着你可以以一种非常独立、受控的方式，尝试各种不同的输入，观察输出的变化，而不受之前的记忆干扰。这种系统性的实验、测试和设计不同方案的能力，就是它的工程属性所在。

**[00:04:15] David:**
我非常同意。而且我想补充的是，从我的角度看，还有一个 **系统集成（Systems Integration）** 的层面。

在企业环境中，提示工程很少仅仅是关于“写一个提示”。它是关于：
* 我们如何将这个提示与数据检索（Retrieval）系统结合？
* 我们需要什么样的数据？
* 我们如何处理延迟（Latency）和成本的权衡？
* 我们如何评估输出？

这实际上是一种 **系统思维（Systems Thinking）**。你需要构建一个围绕模型的复杂系统，而不仅仅是对着聊天框说话。

**[00:04:45] Alex:**
这很有趣。我在推特上经常看到一种说法，说“提示工程不是工程，它只是你可以用代码管理的‘感觉’（Vibes）”。你们对此怎么看？提示是代码吗？

**[00:05:00] David:**
这是个好问题。我认为你不应该试图过度工程化，去构建疯狂的抽象层，或者试图通过代码来“元编程”提示。通常，最有效的方法就是写下非常清晰的任务描述。

但是，**管理提示的过程** 必须像管理代码一样：
* 你需要版本控制（Version Control）。
* 你需要进行回归测试（Regression Testing）。
* 你需要跟踪实验结果。

我们现在处于一个奇怪的范式中：**写一篇好的散文（Prose）和写代码正在变成同一件事**。通过自然语言写作，我们实际上是在对这一块“硅”进行编程。

---

## 2. 什么是优秀的提示工程师？(Qualities of a Good Prompt Engineer)

**[00:06:36] Alex:**
既然我们定义了什么是提示工程，那么，**什么是好的提示工程师？** 如果你要招聘一个人来做这行，或者你自己想变得更好，你需要具备什么特质？Amanda？

**[00:06:47] Amanda:**
我认为有几个关键点：

1.  **清晰的沟通能力**：如果你不能清晰地向另一个人陈述一个概念或任务，你很可能也无法向模型陈述清楚。你需要有能力将复杂的意图分解为明确的指令。

2.  **迭代的意愿（Willingness to Iterate）**：这可能是最重要的。人们通常有一个误区，认为你要像写代码一样，坐下来深思熟虑，写出一个完美的提示，然后按回车，就结束了。
    * 实际上，我在 15 分钟内可能会发送数百个提示。
    * 你需要根据模型的反馈快速调整。你会发现：“哦，我在第 3 行写的那个词误导了它，我要改掉。”或者“它在这里产生了幻觉，我要加一个约束。”
    * 这是一种 **快速循环的实验过程**。

3.  **预判错误与边缘案例（Thinking about Edge Cases）**：这和编程很像。容易犯的错误是只看“快乐路径”（Happy Path，即理想情况）。
    * 优秀的提示工程师会问：“如果输入的数据不符合预期怎么办？”
    * 例如，如果你要求模型提取以 "J" 开头的名字。你需要测试：如果文本里没有名字怎么办？如果文本是空的怎么办？如果全是乱码怎么办？
    * 你需要为这些情况编写具体的处理逻辑，比如告诉模型“如果没有找到，请输出 `<none>`”。

**[00:08:56] David:**
我非常赞同关于边缘案例的看法。在我的工作中，这是最大的挑战之一。当你在 API 中构建应用时，用户输入的真实数据往往是“肮脏”的——充满了拼写错误、没有标点符号、或者是完全不连贯的句子。

你需要有一种推理能力，去想象 **实际流量（Production Traffic）** 是什么样子的，而不仅仅是你在测试时输入的那些完美的句子。你需要通过提示来防御这些糟糕的输入。

**[00:09:33] Zach:**
除此之外，我要强调一点，这听起来可能很傻，但却是最重要的：**阅读模型的输出（Reading Model Outputs）**。

在机器学习中，我们经常说“要在数据中畅游”（Look at your data）。在提示工程中，这意味着你必须仔细阅读模型生成的每一个字。
* 很多时候，人们会写一个很长的提示，里面包含“一步步思考（Think step by step）”的指令。
* 然后模型输出了一大段话。如果你不仔细看，你可能没发现模型实际上 **跳过了** 推理步骤，直接给出了答案，而且答案可能是错的。
* 或者模型虽然给出了正确答案，但它的推理逻辑是完全错误的（这就意味着它下次可能会出错）。
* 你需要像审阅代码一样审阅模型的输出。

**[00:10:43] Alex:**
这听起来似乎提示工程有一种 **心理理论（Theory of Mind）** 的成分。你不仅要考虑模型如何看待你的指令，还要考虑最终用户将如何与模型交互。

**[00:11:06] David:**
是的。这很难。最难的事情之一是将你脑中的知识 **解构（Deconstruct）** 并写下来。
* 作为一个领域专家，你有很多“默认假设”。比如你是医生，你写病例时会有很多缩写和隐含逻辑。
* 好的提示工程师能够退一步，剥离这些假设，清晰地传达完成任务所需的 **全部事实集（Full Fact Set）**。
* 很多时候，人们写的提示之所以失败，是因为提示里充满了只有他们自己（或者是他们公司内部）才懂的行话或假设，而模型对此一无所知。

---

## 3. 调试与信任模型 (Debugging & Trust)

**[00:12:42] Zach:**
说到这里，我经常做的一个调试技巧是：直接问模型。
当我写好一个复杂的提示后，我做的第一件事往往是把它发给模型，然后说：
> “我不希望你执行这些指令。我只想让你阅读这些指令，并告诉我：哪里不清楚？哪里有歧义？如果你是模型，你会觉得哪里很难执行？”

**[00:13:03] Amanda:**
这太棒了。我也会做类似的事。如果模型出错了，你可以直接问它：
> “你做错了。你能看看你为什么错了吗？你能写一个 **修正版** 的指令，让你自己在未来不再犯这个错误吗？”

令人惊讶的是，很多时候模型自己就能给出非常好的修正建议。它会说：“哦，我误解了第 2 条指令，你应该把它改成这样……”。然后你把它的建议贴回去，通常问题就解决了。

**[00:14:13] Alex:**
Amanda，这很有趣。你似乎很信任模型来辅助你的研究工作。作为一个在微调团队工作的人，你是如何建立这种信任的？你如何知道什么时候该信它，什么时候不该信？

**[00:14:55] Amanda:**
这是一个很好的问题。其实，我的默认状态是 **从不信任模型**。我会不断敲打它，试图打破它。

但我看重的是 **高信号（High Signal）**。在传统的机器学习中，我们习惯看大数据集的统计指标（如 Loss 曲线）。但在提示工程中，数据点的质量比数量更重要。
* 如果你有一组精心构建的 100 个提示（涵盖了所有边缘情况、难点、陷阱），这比数千个低质量的随机数据点更有价值。
* 通过仔细阅读模型在这些高难度案例上的表现，你可以学到很多关于它“如何思考”的信息。你可以建立一种直觉：在这个特定的任务上，模型的边界在哪里。


# AI 提示工程深度解析 (Part 2)

## 4. 调试、实验与高信号数据

**[00:15:20] Amanda:**
接上回，我想强调的是：提示本身就是实验成败的关键因素。
* 如果你是一个机器学习工程师，你可能会花几周时间去调优模型架构或数据集。
* 但如果你花了几周时间写代码，却只花了 5 分钟写提示，你的实验可能会失败。
* 而失败的原因可能并不是模型能力不行，仅仅是因为你没有花那额外的 15 分钟把提示写清楚。这就导致了你的整个实验只能得到 **低信号（Low Signal）** 的结果——你不知道是你错了还是模型错了。

**[00:17:13] David:**
是的，在企业应用中也是如此。很多客户会说“模型不行”，然后我就去看他们的提示，发现全是模棱两可的指令。我们稍微修改一下提示，性能就从 60% 提升到了 90%。
这说明：**如果你没有一个好的基准提示（Baseline Prompt），你就无法准确评估模型的真实能力。**

---

## 5. 模型的局限性与“不可能的任务”

**[00:18:23] Zach:**
提示工程的一个巨大陷阱是：总觉得有一个“神话般的完美提示”在等着你。
* 有时候你会陷入一种执念：“只要我换个词，只要我加个‘请’字，它就能做到了。”
* 但有时候，你必须承认：**某些事情是当前的模型做不到的**。

**[00:19:50] Zach 的 Pokémon 案例：**
我想分享一个有点荒谬但很有启发性的失败案例。我曾试图让 Claude 玩 Game Boy 上的《精灵宝可梦红版》（Pokémon Red）。
* **任务**：我把游戏画面的截图喂给模型，问它：“现在的状态是什么？我应该按什么键？”
* **挑战**：这是一个多模态任务，涉及图像识别、状态记忆和策略规划。
* **尝试**：我尝试了极复杂的提示。我在图像上叠加了网格（Grid Overlay），详细描述每个像素块的含义，甚至试图让它把图像转译成 ASCII 地图来辅助理解。
* **结果**：对于某些具体的子任务（比如识别“我现在是不是在和 NPC 对话”），模型就是做不到。因为图像分辨率太低，而且模型没有连续性的记忆（它不知道上一帧发生了什么）。
* **教训**：虽然我失败了，但通过这种极端的压力测试（Stress Testing），我从“完全没信号”变成了“有一点信号”。我了解到模型在处理低分辨率像素艺术（Pixel Art）和连续状态跟踪方面的局限性。这也教会了我 **何时该放弃** 并等待更强的下一代模型。

**[00:22:18] David:**
多模态确实是一个巨大的挑战。Zach，你提到在这个过程中，你对文本提示的直觉（Intuitons）是否能迁移到图像上？

**[00:22:30] Zach:**
很有趣的是，很多文本提示的技巧在图像上并不适用。
* 例如 **少样本提示（Few-shot prompting）**：在文本任务中，给几个例子通常非常有效。但在图像任务中，如果你给它几张“成功玩游戏”的截图作为例子，效果似乎并不如预期那样好。
* 图像的语义太丰富、太复杂了，模型很难从几个例子中提取出你想要的那个特定的“模式”。

---

## 6. 角色扮演 (Role Prompting) vs 诚实

**[00:24:30] Alex:**
这就引出了一个经典的话题：**角色扮演（Role Prompting）**。
我们经常看到这样的建议：“告诉模型：你是一个世界级的 Python 专家”、“你是一个获得诺贝尔奖的物理学家”。
但这真的必要吗？Amanda，我知道你通常非常诚实，甚至会告诉模型“我是一个 AI 研究员，我在测试你”。你对此怎么看？

**[00:25:10] Amanda:**
我觉得随着模型变得越来越强，这种“扮演角色”的需求正在减少。
* 在早期模型（如 GPT-3）中，如果你不给它一个角色，它可能会输出非常通用的、像维基百科一样的文本。
* 但现在的模型（如 Claude 3），如果你直接说：“我要写一个 Python 脚本来做 X”，它就会写得很好。你不需要骗它说它是 Guido van Rossum（Python 之父）。

**[00:25:50] Amanda:**
而且，保持诚实有助于 **清晰的沟通**。
* 如果你在构建一个用于评估模型能力的测试集（Eval），你就直接告诉模型：“我要构建一个评估测试。请生成一些困难的问题。”
* 而不是假装：“你是一个严厉的老师，正在给学生出期末考试题。”
* 模型知道什么是“评估测试”。直接说出你的意图，通常比用复杂的隐喻（Metaphor）绕圈子更有效，也更不容易产生歧义。

**[00:26:36] Zach:**
我同意大部分观点，但我偶尔还是会用隐喻。
* 比如，我想让模型评估一个图表的质量。如果我直接定义“质量”，我很难写全所有的标准（清晰度、美观度、数据准确性等等）。
* 但如果我说：“**如果这是一份高中生的作业，你会给几分？**” 或者 “**如果这是一份发表在《自然》杂志上的图表，你会给几分？**”
* 这种隐喻能迅速调动模型内部关于“高质量”与“低质量”的潜在知识，而不需要我把所有规则都写出来。但这通常是因为 **我们人类很难用语言精确描述我们的标准**，而不是因为模型不懂。

**[00:28:28] David:**
我认为最好的做法通常是 **描述具体的上下文（Context）**。
* 与其说“你是一个有用的助手”，不如说：
    > “你是一个嵌入在这个特定产品（比如税务软件）中的支持聊天窗口。你的用户是那些正在为报税感到焦虑的小企业主。请用专业但安抚性的语气回答。”
* 这比单纯的角色扮演更有用，因为它提供了 **语境（Context）**、**受众（Audience）** 和 **语气（Tone）** 的具体指示。

---

## 7. 像对待“临时工”一样写提示 (The Intern Analogy)

**[00:32:00] Amanda 的“临时工”思维实验：**
这是我最喜欢的思维模型之一。想象你雇了一个非常聪明、刚毕业的大学生来做暑期实习生（临时工）。
* 他很能干，但他 **对你的公司一无所知**。
* 他不知道你的项目背景，不知道你的行话，不知道你的偏好。

**[00:32:20]**
如果你让他写一封电子邮件，你会怎么对他说？
* ❌ 你不会对他喊：“你是一个世界级的邮件撰写专家！现在写！”（这是没用的角色扮演）
* ✅ 你会说：“我们要给客户 X 写封信，目的是 Y。以前我们类似的信是这样写的（给个例子）。注意不要用这种语气，要用那种语气。如果遇到不知道的信息，先空着或者问我。”

**[00:32:45] Amanda:**
**直接把你会对那个实习生说的话写下来，录音，转成文字。**
这通常就是你能写的最好的提示。它包含了背景、目标、约束条件和示例——这些正是模型需要的全部信息。

**[00:33:00] David:**
我是这个方法的超级粉丝。
我经常让客户把他们想让模型做的事情 **口述（Dictate）** 出来。
* 客户往往会陷入“写代码”的模式，试图写出精简、奇怪的指令。
* 但当他们口述时，他们会自然地说：“哦，顺便说一下，如果遇到这种情况，千万别那么做，因为上次那个客户很生气……”
* 这些 **“顺便说一下”的细节**，往往就是提示中缺失的关键 **边缘案例处理（Edge Case Handling）**。把这些录音转录下来，粘贴到提示里，效果往往比他们精心编写的“提示语”好得多。

# AI 提示工程深度解析 (Part 3)

## 8. 思维链 (Chain of Thought) 与格式

**[00:37:04] Alex:**
这就引出了一个非常核心的概念：**思维链（Chain of Thought, CoT）**。
* 有人说，这不仅仅是让模型展示其思考过程，更像是给模型一个 **计算的缓冲空间（Computational Buffer）**。
* 它是真正的推理吗？还是只是增加了 token 数，让模型有时间去计算概率？

**[00:38:23] Zach:**
这是一个很好的问题。
* 不管它是不是人类意义上的“推理”，在实践中，它 **确实有效**。
* 如果你强迫模型直接给出答案，它的错误率会显著上升。
* 如果你让它先写下推理步骤，不仅正确率提高了，你还能检查它的逻辑链条。

**[00:39:00] Zach:**
而且，**结构化（Structuring）** 这种推理非常重要。
* 如果你把推理部分替换成无关的故事或乱码，效果就不行。这说明模型不仅仅是利用了更多的计算时间，它确实在处理与任务相关的信息。
* XML 标签（如 `<thinking>...</thinking>`）在这里非常有用，它能帮助模型区分“这是我在思考”和“这是最终答案”。

**[00:40:49] Alex:**
那么，提示中的 **语法和标点** 重要吗？拼写错误会影响模型表现吗？

**[00:41:04] Zach:**
我个人倾向于保持完美，因为这反映了对细节的关注。如果提示本身充满了错误，模型可能会觉得这是一个低质量的任务。

**[00:43:05] David:**
这其实涉及到 **预训练模型（Pre-trained）** 与 **RLHF 模型（Instruct/Chat）** 的本质区别。
* **预训练模型**：它的目标是预测下一个 token。如果你输入充满拼写错误的文本，它会倾向于预测更多的拼写错误，因为它在模仿这种低质量文本的模式。
* **RLHF 模型**（如 Claude）：它经过了大量的人类反馈微调，被训练成无论输入多么糟糕，都要给出 **有用、无害** 的回答。
    * 所以在现代 Chat 模型中，拼写错误的影响比以前小得多。
    * 但如果你给它大量 Emoji，或者用某种特定的网络俚语，它可能会认为你喜欢这种风格，并调整自己的语气来适应你。这有时是你想要的，有时则不是。

---

## 9. 企业级提示 vs 研究级提示

**[00:45:21] Alex:**
那么，为企业产品写提示，和为研究目的写提示，或者日常聊天写提示，有什么根本区别吗？

**[00:46:00] Zach:**
这主要取决于你的目标是什么：是 **多样性（Variety）** 还是 **可靠性（Reliability）**。

* **研究/探索**：
    * 当我在探索模型能力时，我希望看到多样性。我不希望限制模型的发挥。
    * Amanda 提到过，她不喜欢给太多示例（Few-shot），因为这会限制模型去探索那些示例之外的可能性。
    * 这种时候，我会用 **说明性（Illustrative）** 的例子，甚至是用儿童故事做例子来提取某种逻辑结构，避免模型过拟合于特定的内容格式。

* **企业/产品**：
    * 你非常看重 **一致性（Consistency）** 和 **可靠性**。
    * 你需要模型在数百万次调用中，都能以完全相同的 JSON 格式输出，不能有任何偏差。
    * 在这种情况下，我会使用 **大量的示例（Many-shot）**，甚至多到让人受不了（比如 20 个甚至 50 个例子），只为了确保模型被死死地钉在这个格式上。
    * 即使这意味着牺牲一点创造力，为了稳定性也是值得的。

**[00:49:20] David:**
完全同意。在企业中，**测试（Evaluation）** 是核心。
* 日常聊天：只要这次对了就行。
* 企业应用：你需要构建一个包含各种 **边缘案例** 的测试集，确保提示在整个输入分布上都表现良好。

---

## 10. 提升提示技巧的建议

**[00:51:00] Alex:**
如果要给正在观看的观众一个建议，如何提升他们的提示技巧？

**[00:51:10] Zach:**
**阅读提示，阅读模型输出。**
* 不要只看统计数字。真的去读那几百个 token。去理解模型为什么会在那个特定的地方出错。

**[00:51:30] Amanda:**
**把你的提示给另一个人看（即使他们不懂技术）。**
* 如果他们看不懂你想让模型做什么，那模型很可能也看不懂。
* 还有，**挑战边界**。试着让模型做你认为它做不到的事情。你会惊讶地发现它的能力往往超乎想象。

**[00:52:00] David:**
**给模型一个“出路”（Escape Hatch）。**
* 在提示中明确告诉它：“如果你遇到不确定的情况，或者是没有足够信息的情况，请输出 `<unsure>` 或者 `<missing_info>`。”
* 这不仅能提高回答的准确率（减少幻觉），还能帮你 **清洗数据**。你可以通过搜索这些标签来快速找到那些有问题的输入。

---

## 11. 越狱 (Jailbreaking) 与历史

**[00:54:00] Alex:**
我们经常听到关于 **越狱（Jailbreaking）** 的故事。这到底发生了什么？

**[00:54:40] Amanda:**
越狱本质上是一种 **对抗性攻击（Adversarial Attack）**。
* 很多时候是将模型推离其 **训练分布（Distribution）**。
* 或者利用模型训练中的某些特性。比如早期的 GPT 模型，如果你让它用 Base64 编码回答，或者用祖鲁语（Zulu）开始回答，它可能会绕过英语的安全过滤器。
* 这是一种混合了 **黑客思维** 和 **社会工程学** 的技巧。你试图找到模型防御机制的漏洞。

**[00:57:00] 提示工程的历史演变**：
* 以前有很多“黑客技巧”（如必须说“Think step by step”或者“Take a deep breath”）。
* 现在，许多这样的技巧已经被 **内化（Internalized）** 到模型训练过程中了。
    * 例如，Claude 在做数学题时，如果它觉得难，它会自动倾向于逐步思考。
* **趋势**：随着模型变强，我们对它表现出更多的 **“尊重”**。
    * 我们不再需要像哄小孩一样哄它（Babying the model）。
    * 我们现在的提示更像是给同事写备忘录：直接、专业、包含复杂的上下文。
    * 我们可以给它整篇论文，让它阅读并执行复杂的分析，而不需要把任务分解成几十个小步骤。

---

## 12. 提示工程的未来

**[01:04:35] Alex:**
最后，一个大问题：**未来还需要提示工程师吗？** 随着模型越来越聪明，这个职业会消失吗？

**[01:05:00] Zach:**
我认为 **“明确目标的设定（Specifying the goal）”** 永远是必要的。
这实际上是一个 **信息论** 的问题：你需要提供足够的信息（比特）来通过通信渠道，让模型知道你想要什么。只要这种信息传输的需求存在，提示工程就不会消失。
变化的将是：**工具会进化**。
* Claude 会更多地帮助我们写提示。
* 提示本身可能会变成一种更高级的交互形式。

**[01:09:26] Amanda:**
未来的交互可能更像 **咨询（Consulting）**。
* **现在的范式**：像对待临时工。你需要给出所有指令，事无巨细。
* **未来的范式**：像对待 **专家设计师**。
    * 你只要说：“我要个海报。”
    * 模型不会傻傻地做一个通用的海报。它会反问你：
        > “好的。你想表达什么主题？受众是谁？偏好什么风格？需要包含哪些具体信息？”
    * 这叫做 **启发（Elicitation）**。模型将通过采访你，从你脑中提取必要的信息来构建完美的提示。

**[01:13:21] Amanda:**
所以，提示工程的核心并不是“写出神奇的咒语”。
它的核心是 **内省（Introspection）** 和 **将大脑外部化（Externalize your brain）**。
这就像哲学写作或法律起草：
* 你需要为一个受过良好教育但对你的特定话题一无所知的人（Educated Layperson）写作。
* 你需要把脑中那些复杂的、隐含的、只可意会不可言传的知识，**清晰、明确、无歧义** 地转化为文字。
* 只要这种“将意图转化为精确指令”的需求存在，提示（或者说与模型的高效沟通）就会存在，无论它叫不叫“工程”。

---

## 关键术语表 (完整版)

| 英文 | 中文 | 说明 |
|------|------|------|
| **Prompt Engineering** | 提示工程 | 设计和优化输入文本以引导 AI 模型生成所需输出的技术。 |
| **Edge Cases** | 边缘案例 | 数据或任务中不常见、极端或意外的情况，容易导致模型出错。 |
| **Few-shot / Many-shot** | 少样本/多样本提示 | 在提示中提供少量或大量示例来教模型如何执行任务。 |
| **Chain of Thought (CoT)** | 思维链 | 要求模型在给出最终答案前展示推理步骤，通常能提高准确性。 |
| **Pre-trained Model** | 预训练模型 | 仅经过海量文本训练，主要能力是文本补全的模型，未经过 RLHF。 |
| **RLHF (Reinforcement Learning from Human Feedback)** | 基于人类反馈的强化学习 | 经过微调以符合人类偏好（有用、诚实、无害）的模型。 |
| **Theory of Mind** | 心理理论 | 理解模型（或他人）的认知状态、意图和知识局限性的能力。 |
| **System Thinking** | 系统思维 | 将提示视为整个软件系统的一部分，考虑数据流、延迟和可靠性。 |
| **Elicitation** | 启发/诱出 | 通过提问从用户那里获取隐含知识或明确需求的过程。 |
| **Jailbreaking** | 越狱 | 通过对抗性提示绕过模型的安全限制或行为准则。 |
| **Adversarial Attack** | 对抗性攻击 | 故意设计输入以导致模型出错或表现异常。 |
| **Escape Hatch** | 出路/逃生舱 | 在提示中给模型一种“不知道”或“无法回答”的选项，避免幻觉。 |