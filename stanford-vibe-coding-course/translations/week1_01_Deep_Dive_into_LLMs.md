---
原文链接: https://www.youtube.com/watch?v=7xTGNNLPyMI
原文标题: Deep Dive into LLMs like ChatGPT
讲师: Andrej Karpathy
所属周次: Week 1-1
阅读时间: 约 25 分钟 (Part 1)
优先级: ⭐必读
翻译日期: 2026-02-02
当前进度: Part 1 / 5
---

# 像 ChatGPT 这样的大型语言模型深度解析 (Part 1)

## 0. 引言：我们需要什么样的心理模型？

**[00:00:00]** 大家好。我想制作这个视频已经有一段时间了。这是一个关于像 ChatGPT 这样的 **大型语言模型（Large Language Models, LLMs）** 的入门介绍。我希望这个视频能成为该主题的某种“权威指南”。

**[00:00:15]** 我的目标受众是普通大众。我的目的是为你提供一种 **心理模型（Mental Model）**，让你知道当你使用这些工具时，到底发生了什么。显然，在某些方面它们是神奇和令人惊叹的，它们在某些事情上做得非常好；但在另一些事情上，它们不仅不擅长，甚至可以说是失败的。它们有很多“锋利的边缘”，有很多你需要注意的地方。

**[00:00:35]** 那么，在这个文本框背后到底是什么？你可以在里面输入任何东西并按回车，但我们应该输入什么？生成的这些词是如何回来的？这是如何工作的？你到底在和谁说话？

**[00:00:46]** 我希望在这个视频中探讨所有这些话题。我们将通过整个流程来了解这些东西是如何构建的，我希望带你通过训练一个现代 LLM 的每一个阶段，但我会保持一切对普通观众来说都是通俗易懂的。首先，让我们看看如何构建像 ChatGPT 这样的东西。

---

## 1. 预训练阶段 (Pre-training)：这一过程的起点

**[00:01:04]** 让我们来构建 ChatGPT。这将涉及按顺序排列的多个阶段。第一个阶段称为 **预训练（Pre-training）** 阶段。

**[00:01:14]** 预训练阶段的第一步非常直观：我们需要下载并处理互联网。为了让你对这大概是什么样子有个概念，我建议看看 Hugging Face 这家公司最近发布的一个名为 "FineWeb" 的数据集。他们在博客文章中详细介绍了如何构建 FineWeb 数据集。

**[00:01:34]** 所有主要的 LLM 提供商，如 OpenAI、Anthropic、Google、Meta 等，内部都会有某种等同于 FineWeb 数据集的东西。

### 1.1 数据收集：我们需要什么？

**[00:01:42]** 大致来说，我们要实现什么？我们试图从公开来源获取互联网上的大量文本。我们有两个核心目标：
1.  **海量（Large Quantity）**：我们需要极其庞大的文本量。
2.  **高质量与多样性（Quality & Diversity）**：我们希望这些文档质量很高，涵盖各种主题（语言、编码、历史、物理、化学、生物等）。因为我们希望这些模型内部包含这些知识。

**[00:02:14]** FineWeb 是一个在生产级应用中相当有代表性的数据集，实际上最终只占用了大约 44 TB 的磁盘空间。如果你仔细想想，这在今天并不是一个不可想象的巨大数据量。你可以去商店买大约 5 到 10 个硬盘，就能装下用来训练这些模型的整个互联网文本。

**[00:02:35]** 我们处理的是纯文本，并且我们进行了非常积极的过滤，只保留高质量的部分。

### 1.2 数据来源：Common Crawl

**[00:02:53]** 许多此类工作的起点是来自 **Common Crawl** 的数据。Common Crawl 是一个非营利组织，他们自 2007 年以来一直在抓取互联网。截至 2024 年，他们已经累积了巨大的抓取数据。

**[00:03:08]** 他们使用这种“网络蜘蛛”在互联网上漫游。你从几个种子网页开始，跟随所有链接，不断索引所有信息，随着时间的推移，你最终会得到大量的互联网数据。这就是所有这些数据集的起点。

**[00:03:22]** 但是，Common Crawl 的原始数据质量是非常低的，甚至可以说是“可怕的”。你需要通过一个复杂的管道（Pipeline）来处理这些数据。

### 1.3 数据清洗管道 (The Pipeline)

**[00:03:40]** 让我们看看 FineWeb 是如何处理这些数据的：

1.  **URL 过滤 (URL Filtering)**：
    使用黑名单过滤掉不需要的域名。例如，你可能不希望抓取恶意软件网站、垃圾邮件网站、纯营销网站。对于某些模型，你可能还想过滤掉成人内容（NSFW）。

2.  **文本提取 (Text Extraction)**：
    网页是原始的 HTML，包含大量的标记、JavaScript、CSS 等。我们需要提取纯文本。你需要去除导航栏、侧边栏、页脚等无关内容，只保留文章的主体文本。

3.  **语言过滤 (Language Filtering)**：
    Common Crawl 包含各种语言。例如，FineWeb 做了一个设计决策，他们使用语言分类器，只保留被识别为英语且置信度超过 65% 的网页。
    *注意：这并不意味着我们不喜欢其他语言，但这通常是为了让模型在特定语言（如英语）上表现更好而做出的权衡。如果你过滤掉所有西班牙语，模型稍后在西班牙语上的表现自然会不佳。*

4.  **个人身份信息移除 (PII Removal)**：
    我们需要移除电子邮件地址、社会安全号码等敏感信息。

5.  **去重 (Deduplication)**：
    互联网上有很多重复内容。我们需要识别并移除这些重复项，以免模型死记硬背或过拟合。

**[00:06:24]** 经过所有这些步骤，最终我们得到了像 FineWeb 这样的数据集。如果你查看这些数据，它们看起来就是纯文本文件。例如，这里有一篇关于 2012 年肯塔基州龙卷风的新闻文章，或者是一篇关于肾上腺的详细医学文章。

**[00:06:55]** 这里的关键点是：我们现在拥有了大量多样化的文本，总计约 44 TB。这是我们下一阶段的起点。

---

## 2. 词元化 (Tokenization)：将文本转化为数字

**[00:07:05]** 为了给你一个直观的感觉，我编写了一个小脚本，取了 FineWeb 数据集的前 200 个文档并将它们连接在一起。我们得到了一块巨大的原始文本“挂毯”。

**[00:07:30]** 接下来，我们希望训练一个神经网络来内化和模拟这些文本。但在将文本输入神经网络之前，我们需要解决一个问题：**如何表示这些文本？**

### 2.1 为什么不能直接用字符？

**[00:07:54]** 神经网络不仅是一个复杂的数学结构，它期望的输入必须是数字，或者具体来说，是 **一维的符号序列**。并且，我们需要一组 **有限的可能符号**（词表）。

**[00:08:36]** 计算机底层存储文本使用的是 UTF-8 编码，也就是 0 和 1 的序列，或者是字节（Bytes）序列。
* 如果我们使用 0 和 1，序列会变得非常长。序列长度（Context Length）在神经网络中是一种非常昂贵的资源，我们不能浪费它。
* 如果我们使用字节（0-255），我们有 256 个可能的符号。这比二进制好，但在最先进的语言模型中，我们希望进一步压缩序列长度。

**[00:09:43]** 我们面临一个权衡：
1.  **词表大小 (Vocabulary Size)**：你可以拥有的独特符号的数量。
2.  **序列长度 (Sequence Length)**：表示同一段文本所需的符号总数。

我们希望在保持词表大小可控的前提下，尽可能缩短序列长度。

### 2.2 字节对编码 (Byte Pair Encoding, BPE)

**[00:11:01]** 行业标准的解决方案是运行一种叫做 **字节对编码（BPE）** 的算法。
它的工作原理是：
1.  从原始字节开始。
2.  寻找非常常见的连续字节对（例如 `t` 和 `h` 经常一起出现变成 `th`）。
3.  将它们合并成一个新的符号，并在词表中为其分配一个新的 ID。
4.  不断迭代这个过程，直到达到预定的词表大小。

**[00:11:30]** 实际上，大约 **100,000 个** 可能的符号是一个很好的甜蜜点（Sweet spot）。
* **GPT-4** 具体使用了 **100,277** 个符号。
* **Llama 3** 使用了 **128,000** 个符号。

### 2.3 Token 的实际样子

**[00:12:00]** 这种从原始文本转换为这些符号序列的过程称为 **词元化（Tokenization）**。这些符号被称为 **词元（Tokens）**。

**[00:12:25]** 让我们看一个演示。我推荐使用 `tiktokenizer.vercel.app`。
* 输入：`Hello World`
* 结果：这被分解为两个 Token。
    * `Hello` -> ID 15339
    * ` world` -> ID 1917 (注意 `world` 前面的空格通常包含在单词的 Token 中)

**[00:13:00]** 关键点：
* **区分大小写**：`hello` 和 `Hello` 是完全不同的 Token，有不同的 ID。
* **空格处理**：空格通常是 Token 的一部分。
* **非英语语言**：对于像韩语这样的语言，因为它们在训练数据中出现较少，BPE 算法合并得不够好，导致每个字可能需要更多的 Token（即序列更长，处理成本更高）。这是一个重要的问题，意味着某些语言使用 LLM 的成本更高。

**[00:14:50]** 所以，最终在神经网络看来，它看到的不是“文本”，而是 **Token ID 的整数序列**。例如：`[15339, 1917, 0, ...]`。这就是我们输入给模型的东西。

---

## 3. 神经网络训练 (Neural Network Training)

**[00:15:21]** 现在进入最有趣的部分：神经网络训练。
我们在这一步要做的是：模拟这些 Token 在序列中相互跟随的统计关系。

### 3.1 训练设置：预测下一个 Token

**[00:15:40]** 想象我们有一个 **上下文窗口（Context Window）**。
在训练过程中，我们从庞大的数据集中随机选取一段。假设我们的窗口大小仅为 4 个 Token（实际上通常是 4096, 8192 或更多）。

* **输入**：`the`, `cat`, `sat`, `on` (对应的 Token ID 序列)
* **目标**：预测第五个 Token 是什么？

**[00:16:00]** 神经网络接收这 4 个整数作为输入。它的输出是什么？
它的输出是对下一个 Token 的预测。具体来说，它输出 **100,277 个数字**（对应词表大小）。每个数字代表该 Token 成为“下一个 Token”的 **概率**。

### 3.2 初始化与迭代

**[00:17:14]** 1.  **初始状态**：也就是“婴儿”神经网络。它的参数是随机初始化的。如果你输入 `the cat sat on`，它输出的概率分布是完全混乱均匀的，它可能会预测下一个词是 `refrigerator` 或 `sky`，完全随机。
2.  **标签（Label）**：但是，因为我们是在训练集上进行训练，我们 **知道** 正确答案是什么。在原句中，`the cat sat on` 后面是 `a`。
3.  **计算损失（Loss）**：我们比较模型的预测（随机垃圾）和事实（`a`）。我们计算一个叫做“损失”的数学指标，它衡量模型错得有多离谱。
4.  **更新参数**：这是魔法发生的地方。我们使用一种叫做 **反向传播（Backpropagation）** 的算法，精确地计算如何调整神经网络内部的每一个参数（权重），使得下次输入 `the cat sat on` 时，`a` 的概率稍微高一点点，而其他词的概率稍微低一点点。

**[00:19:00]** 这就是训练的全部内容。我们在数十亿甚至数万亿的 Token 上，重复这个过程数万亿次。我们不断地给它看片段，让它预测，计算错误，调整参数。

### 3.3 神经网络内部结构 (Transformer)

**[00:20:19]** 让我们稍微打开一下这个“黑盒子”。这个神经网络到底是什么？
现代 LLM 都使用一种特定的架构，称为 **Transformer**（由 Google 在 2017 年提出）。

**[00:21:00]** 即使是像 Llama 3 405B 这样巨大的模型，其定义代码也非常简短。如果你去掉了注释和空行，定义整个神经网络架构的代码可能只有几百行 Python 代码。

**[00:22:00]** * **参数（Parameters/Weights）**：这些是模型的“知识”存储在的地方。它们只是存储在计算机内存中的数字。对于 GPT-2，有 15 亿个参数；对于 Llama 3，有 4050 亿个参数。
* **运算**：神经网络不仅仅是参数，它是一种数学表达式。输入数据（Token）进入网络，与这些参数进行大量的加法、乘法、指数运算。

**[00:23:19]** 你可以把这想象成信息流。Token ID 进入底部，经过层层处理（Block 1, Block 2, ... Block 96...），每一层都在更新和精炼这些信息，直到最后顶部输出预测概率。

**[00:24:00]** 有人喜欢把这比作大脑。参数是突触连接的强度，神经元是计算单元。虽然这种类比有一定道理，但人工神经网络非常规则、简单且数学化，与生物大脑的复杂性相比其实相去甚远。它们没有“生物记忆”，它们只是一个巨大的、固定的数学表达式，我们将数据推过这个表达式。

---

## 4. 推理阶段 (Inference)：生成文本

**[00:26:13]** 假设我们已经训练好了模型，现在我们进入 **推理（Inference）** 阶段。这是你使用模型生成新文本的时候。

**[00:26:26]** 这是一个循环过程：
1.  **输入**：给模型一个提示（Prompt），例如 `The`.
2.  **预测**：模型输出下一个 Token 的概率分布。比如它说 `cat` 的概率是 30%，`dog` 是 20% 等。
3.  **采样 (Sampling)**：这是一个关键步骤。我们不仅仅选概率最高的那个（虽然可以这么做，称为 Greedy Decoding），通常我们会根据这个概率分布“掷骰子”。这意味着即使 `cat` 概率最高，我们也可能偶尔选中 `dog`。这赋予了模型 **创造性** 和 **多样性**。
4.  **循环**：假设我们选中了 `cat`。我们将 `cat` 加回到输入中，现在输入变成了 `The cat`。我们再次将其喂给神经网络，预测下一个 Token。

**[00:28:00]** 这个过程不断重复，直到模型生成一个特殊的 **结束 Token**（End of Text Token），或者达到最大长度。

**[00:29:57]** 总结一下第一阶段（预训练）：
* 我们得到了一大堆互联网文本。
* 我们将文本转化为 Token 序列。
* 我们训练一个 Transformer 神经网络来预测序列中的下一个 Token。
* 训练后的产物称为 **基座模型（Base Model）**。


# 像 ChatGPT 这样的大型语言模型深度解析 (Part 2)

## 5. 案例研究：GPT-2 与 Llama 3

**[00:31:17]** 为了让这些抽象概念更具体，我想详细介绍两个特定的模型。首先是 OpenAI 在 2019 年发布的 **GPT-2**。它是现代技术栈的雏形，几乎定义了后来所有大模型的基础架构。

### 5.1 GPT-2 (2019)：现代 LLM 的鼻祖

**[00:31:35]** GPT-2 的一些关键统计数据：
* **参数量 (Parameters)**：15 亿 (1.5B)。这在当时是巨大的，但以今天的标准看非常小。
* **词表大小 (Vocab Size)**：50,257 个 Token。
* **上下文长度 (Context Length)**：1024 个 Token。这意味着它只能“看”到大约 750 个单词的上下文。
* **训练数据**：约 1000 亿个 Token。

**[00:33:19]** 我曾尝试复现 GPT-2（项目名为 `llm.c`）。
* **训练成本**：在 2019 年，训练这样一个模型的计算成本约为 **40,000 美元**。
* **今日成本**：得益于 GPU 硬件（如 NVIDIA H100）和软件算法的巨大进步，今天训练同样的 GPT-2 模型成本不到 **100 美元**。这展示了技术进步的惊人速度。

**[00:34:54]** **训练过程可视化**：
我在训练过程中记录了一个关键指标：**损失（Loss）**。
* **Loss** 是模型预测下一个 Token 的平均负对数概率。简单来说，Loss 越低，模型预测越准确。
* 你可以看到 Loss 曲线随着训练步数稳步下降。这是我们唯一关心的指标。我们就像看着油漆变干一样看着 Loss 下降。

### 5.2 算力需求与 GPU 集群

**[00:39:32]** 训练这些模型需要巨大的算力。我在云端使用了包含 8 个 **NVIDIA H100 GPU** 的节点。
* **并行计算**：这些 GPU 非常适合这种大规模并行矩阵乘法运算。
* **显存带宽**：H100 拥有巨大的显存带宽（3.35 TB/s），这对于快速移动大量数据至关重要。

**[00:41:00]** 这也是为什么 NVIDIA 的市值如此之高。所有科技巨头（Google, Meta, Microsoft, OpenAI）都在疯狂争夺这些 GPU，建造拥有数万甚至数十万个 GPU 的超级计算机集群，只为了训练更大的模型。

### 5.3 Llama 3 (2024)：当代的巨兽

**[00:45:51]** 让我们快进到 2024 年，看看 Meta 发布的 **Llama 3**。
* **参数量**：4050 亿 (405B)。这是 GPT-2 的 270 倍。
* **词表大小**：128,000 个 Token。
* **上下文长度**：8,192 或 128,000 个 Token（取决于版本）。现在的模型可以处理整本书的内容。
* **训练数据**：**15 万亿** (15T) 个 Token。这是 GPT-2 数据的 150 倍。

**[00:46:30]** 训练这样一个模型需要一个由 **16,000 个 H100 GPU** 组成的集群，连续运行数月。成本高达 **数亿美元**。这是一个巨大的工程奇迹。

---

## 6. 基座模型 (Base Model)：不仅仅是自动补全

**[00:47:05]** 训练结束后，我们得到了 **基座模型（Base Model）**。
**🔴 重点**：基座模型 **不是** 助手（Assistant）。如果你直接与基座模型交互，它不会像 ChatGPT 那样回答你的问题。

### 6.1 文档补全机 (Document Completer)

**[00:47:30]** 举个例子，如果你问基座模型：
> "What is the capital of France?" (法国的首都是哪里？)

它可能不会回答 "Paris"。相反，它可能会补全为：
> "What is the capital of Germany? What is the capital of Spain?"

**[00:48:00]** 为什么？因为它是通过模仿互联网文档来训练的。在互联网上，如果你看到一个问题列表，接下来的内容很可能还是问题。它只是在做它被训练去做的事：**预测下一个 Token**。它不知道你想让它回答问题，它以为你在写考卷。

### 6.2 压缩与知识 (Compression is Intelligence)

**[00:50:42]** 尽管基座模型本身不是助手，但它在预测下一个 Token 的过程中，被迫学到了关于世界的 **深层知识**。
* 想象一下，要预测一句关于化学反应的文本的下一个词，模型必须在某种程度上“理解”化学原理。
* 要预测一段代码的下一个 Token，模型必须“学会”编程语法和逻辑。

**[00:51:30]** 我们可以把基座模型看作是对互联网的一个 **有损压缩（Lossy Compression）**。
* **Zip 文件**：这是无损压缩。你可以完全还原原始文件。
* **LLM**：这是有损压缩。我们不能精确还原整个互联网，但我们捕捉到了其中的 **统计规律、知识和逻辑**。这些都被压缩在了 4050 亿个参数中。

### 6.3 模型的两种行为模式

**[00:52:06]** 由于这种压缩特性，模型表现出两种截然不同的行为：

1.  **死记硬背 (Regurgitation / Memorization)**：
    如果你给它一段非常著名、在训练数据中出现过成千上万次的文本（比如《独立宣言》的第一句，或者常用的代码片段），模型几乎可以完美地逐字背诵出来。
    * *原理*：这些数据的概率分布非常尖锐，模型非常确信下一个词是什么。

2.  **幻觉 (Hallucination)**：
    如果你问它一个在训练数据中不存在的、模糊的或极其具体的事实（比如“2024年5月12日《纽约时报》头条是什么？”——假设训练数据截止到2023年），模型会发生什么？
    * *行为*：模型不会说“我不知道”。它会查看它的概率分布，发现没有一个确定的答案，然后它会 **采样（Sample）** 一个看起来合理的答案。
    * *结果*：它会编造一个听起来非常像《纽约时报》头条的句子，但内容完全是虚构的。这就是 **幻觉**。

**[00:55:00]** **🔴 重点**：幻觉并不是模型的“错误”，而是它的 **特性**。它只是在做概率性的文档补全。当它不知道确切答案时，它会模仿答案的 **形式**（Form），而不是 **内容**（Content）。对于创造性写作（写诗、编故事）来说，这是 **创造力（Creativity）**；对于事实性查询来说，这就是 **谎言（Lying）**。

### 6.4 上下文学习 (In-Context Learning)

**[00:56:07]** 尽管基座模型不是助手，但我们可以通过 **提示工程（Prompt Engineering）** 来“诱导”它表现得像个助手。

**[00:56:30]** **少样本提示 (Few-shot Prompting)**：
如果你想让它做翻译，不要直接问。而是给它几个例子：
> English: Hello -> French: Bonjour
> English: Cat -> French: Chat
> English: Dog -> French:

当你输入到这里，模型会识别出这个模式（Pattern），并补全 `Chien`。
它不需要更新任何参数，它只是利用上下文中的信息来推断任务。这称为 **上下文学习**。

**[00:58:00]** 你甚至可以构建一个“假对话”：
> User: Hello, who are you?
> Assistant: I am a helpful AI assistant.
> User: What is 2+2?
> Assistant:

通过这种方式，你可以强行让基座模型进入“助手模式”。但这只是权宜之计。

**[00:59:00]** 总结第二部分：
* 我们花费巨资训练了一个巨大的“互联网模拟器”。
* 它包含世界知识，但它的原生界面是“文档补全”。
* 它会死记硬背常见内容，也会对未知内容产生幻觉。
* 我们可以通过 Prompt 来引导它，但要真正拥有像 ChatGPT 这样的产品，我们需要进入下一个阶段：**后训练（Post-training）**。

# 像 ChatGPT 这样的大型语言模型深度解析 (Part 3)

## 7. 后训练阶段 (Post-training)

**[01:00:01]** 我们现在有了一个巨大的基座模型。它是聪明的，因为它内化了大量的世界知识，但它不是很有用，因为它只能做文档补全。
**目标**：我们希望把这种“文档模拟器”转变成一个 **乐于助人的助手（Helpful Assistant）**。

### 7.1 监督微调 (Supervised Fine-Tuning, SFT)

**[01:01:07]** 这个过程的第一步称为 **监督微调（Supervised Fine-Tuning, SFT）**。
* **数据来源**：我们需要收集一个新的数据集，这次是 **对话数据（Conversational Data）**。
* **人工标注**：我们雇佣大量的人类承包商（Labelers）。我们给他们具体的指示：
    > "请扮演一个助手。用户会问你问题，你要给出有帮助、无害且真实的回答。"

**[01:02:43]** 这样我们就得到了成千上万个对话样本：
> User: Can you help me write a Python script?
> Assistant: Sure! Here is a Python script that does X...

**[01:04:00]** **隐式编程**：
我们将基座模型的互联网数据替换为这个高质量的对话数据集，并在其上 **继续训练**。
* 注意：这个阶段非常快。相比预训练数月的时间，SFT 可能只需要几个小时到几天。
* 结果：模型迅速学会了这种对话模式。它不再试图补全文档，而是学会了在看到“User:”后预测“Assistant:”的内容。

### 7.2 它是谁？人类标注员的统计模拟

**[01:16:58]** **🔴 重点**：当你在使用 ChatGPT 时，你到底在和谁说话？
* 你并不是在和一个有自主意识的 AI 说话。
* 你是在和一个 **普通人类标注员的统计平均值** 说话。

**[01:18:00]** 如果你问一个问题，模型会检索它在训练期间见过的类似对话，并模仿那些人类标注员会怎么回答。
* 如果训练数据中的标注员都很礼貌，模型就会很礼貌。
* 如果标注员喜欢用 emojis，模型也会用 emojis。
* 模型的“性格”完全取决于 SFT 阶段的数据集。

### 7.3 幻觉 (Hallucinations) 的根源

**[01:20:35]** 为什么模型会胡说八道？
* **训练数据的偏见**：在 SFT 数据集中，助手总是能回答问题。标注员被指示要提供帮助，所以所有的训练样本都是“User 问 -> Assistant 答”。
* **自信的模仿**：当遇到它不知道的问题（例如“Orson Kovats 是谁”，一个完全虚构的名字）时，模型没有见过“我不知道”这种回答模式。它的本能是：模仿它见过的自信回答的 **格式**。
* **结果**：它会自信地编造一个关于 Orson Kovats 的传记。

---

## 8. 缓解幻觉与工具使用

**[01:25:50]** 我们如何解决这个问题？

### 8.1 方案一：教会模型说“不知道”

**[01:26:00]** Meta 在 Llama 3 中尝试了一种方法：
1.  **探测知识**：首先探测基座模型，看它到底知不知道某些事实（通过多次提问并检查一致性）。
2.  **构建拒答数据**：如果模型不知道，我们就人工构建一个对话样本，让助手回答：“对不起，我不知道这个信息。”
3.  **微调**：在这些数据上微调模型。

**[01:28:00]** 结果：模型学会了将内部的 **不确定性（Uncertainty）** 与 **拒绝回答** 关联起来。现在的模型在面对它确实不知道的问题时，更倾向于说“我不知道”，而不是编造。

### 8.2 方案二：工具使用 (Tool Use) / 搜索 (Search)

**[01:32:24]** **更聪明的做法**：与其说不知道，不如让模型去查！
* 现代 LLM 被赋予了 **工具使用能力**。
* 如果你问“现在旧金山的天气怎么样？”，模型知道它自己的训练数据截止于过去，无法回答。
* **触发工具**：模型会生成一个特殊的 **搜索 Token**（例如 `<search_query> weather in SF`）。

**[01:34:00]** **系统介入**：
1.  系统检测到这个 Token。
2.  暂停模型生成。
3.  运行 Google 搜索。
4.  将搜索结果（比如来自 Weather.com 的文本）**粘贴** 到模型的上下文窗口中。
5.  **恢复生成**：让模型继续回答。现在，它的上下文中有了正确的天气信息，它可以准确地回答你了。

**[01:35:16]** **工作记忆 (Working Memory)**：
* 模型的 **参数知识** 是模糊的、不准确的（类似于人类的长期记忆，可能记错）。
* 模型的 **上下文窗口** 是清晰的、可直接访问的（类似于人类的工作记忆）。
* **RAG (Retrieval-Augmented Generation)**：这就是所谓的“检索增强生成”。把信息放在上下文中，模型就能像你拿着书开卷考试一样，准确率大大提高。

---

## 9. 模型的认知局限：它如何思考？

**[01:41:51]** 关于模型的自我认知：
* 如果你问“你是谁”，它回答“我是 ChatGPT，由 OpenAI 开发”。
* 这不是因为它有自我意识。这是因为在系统消息（System Message）或 SFT 数据中，我们硬编码了这种回答。如果我微调它说“我是土豆”，它就会坚信自己是土豆。

### 9.1 模型需要 Token 来思考 (Models need tokens to think)

**[01:47:01]** 这是一个极其重要的概念。
* **Transformer 的计算限制**：对于每个输出 Token，模型只能进行固定量的计算（由层数决定）。它不能在生成一个 Token 的瞬间进行无限的思考。
* **类比**：如果我问你“17 x 24 是多少”，你不能马上说出答案。你需要纸和笔，写下中间步骤。

**[01:50:37]** **错误做法**：
> User: "17 x 24?"
> Assistant: "408" (模型试图直接跳到答案，很容易出错)

**[01:52:00]** **正确做法（思维链 CoT）**：
> User: "17 x 24? Let's think step by step."
> Assistant: "Okay. 10 x 24 is 240. 7 x 20 is 140. 7 x 4 is 28. 240 + 140 + 28 = 408."

**🔴 重点**：通过生成更多的 Token（中间步骤），模型实际上是在 **购买更多的计算时间**。每个 Token 都是一次计算机会。如果你强迫它直接给出答案，你是在限制它的智力。

### 9.2 Tokenization 带来的“眼盲”

**[01:58:23]** 很多用户发现 LLM 在简单的拼写或字符任务上表现很差。
* 例如：问 "Strawberry" 有几个 "r"？
* 模型可能会答错。

**[02:00:00]** 原因：**Tokenization**。
* 模型看到的不是 `S-t-r-a-w-b-e-r-r-y` 这些字母。
* 它看到的是 Token ID，例如 `[Strawberry]` (一个整体 ID)。
* 它根本“看”不到里面的字母结构。它必须依赖于它在训练中学到的关于该单词拼写的统计相关性。

**[02:05:00]** **解决方案**：
1.  **让它写代码**：让 Python 脚本去数。代码是字符级的，执行结果是准确的。
2.  **强制分隔**：如果你问 `S t r a w b e r r y` (加空格)，这就迫使每个字母成为单独的 Token，模型就能数对了。


# 像 ChatGPT 这样的大型语言模型深度解析 (Part 4)

## 10. 强化学习 (Reinforcement Learning, RL)：最后的拼图

**[02:10:10]** 我们已经完成了预训练（基座模型）和监督微调（SFT 助手模型）。但这还不是全部。现代最先进的模型都会经历第三个主要阶段：**强化学习（Reinforcement Learning, RL）**。

### 10.1 上学的类比

**[02:11:14]** 我们可以把这三个阶段类比为人类的学习过程：
1.  **预训练 (Pre-training)** = **阅读教科书**。
    你阅读了所有的解释性文本，获取了庞大的知识库，但这只是被动接受。
2.  **监督微调 (SFT)** = **看老师解题**。
    你观察专家（人类标注员）如何解决问题，并试图模仿他们的步骤和格式。你学会了基本的解题套路。
3.  **强化学习 (RL)** = **做练习题**。
    在这个阶段，老师不再给你看过程，而是给你一个问题，让你自己尝试解答。如果你的最终答案是对的，你会得到奖励；如果是错的，你会得到惩罚。你需要自己去摸索 **如何思考** 才能得出正确答案。

### 10.2 为什么需要 RL？

**[02:14:46]** SFT 阶段有一个根本性的问题：
* **人类的思维方式** 可能并不是 **LLM 的最佳思维方式**。
* 人类标注员写的“思维过程”可能非常跳跃、直觉化，或者包含很多隐含假设。
* LLM 需要一种更机械、更逐步、更符合其自身统计特性的思维路径。

**[02:20:21]** 在 RL 中，我们通过 **试错（Trial and Error）** 来发现这种路径。
* 我们给模型一个数学题。
* 让它生成 10,000 个不同的解答路径（采样）。
* 检查哪些路径最终得出了正确答案。
* **强化** 那些成功的路径，**抑制** 那些失败的路径。

**[02:25:00]** 结果：模型开始学会一种甚至连人类都想不到的、但对自己极其有效的解题策略。

---

## 11. 案例研究：DeepSeek R1 与“思考”模型

**[02:28:53]** 最近，中国的 DeepSeek 公司发布了一篇非常重要的论文，介绍了他们的 **DeepSeek R1** 模型。这篇论文重新点燃了业界对纯强化学习（Pure RL）的兴趣。

### 11.1 纯 RL 的威力

**[02:30:35]** DeepSeek 做了一个大胆的实验：
* **不使用 SFT 数据**（或者只用极少量的冷启动数据）。
* 直接在一个基座模型上运行大规模的 RL 训练。
* **奖励函数** 非常简单：只要最终答案是对的（数学题答案、代码通过测试），就给奖励。

**[02:32:00]** **结果惊人**：
* 随着 RL 训练步数的增加，模型在数学和代码任务上的准确率持续上升，甚至超越了经过大量 SFT 的模型。
* 更重要的是 **定性结果**：模型学会了 **思维链（Chain of Thought）**。

### 11.2 涌现的思维链

**[02:34:00]** 没有人教模型要“一步步思考”。
* 为了获得奖励（答对题），模型发现：如果它生成更多的 Token，进行更长时间的推理、自我反思、验证和纠错，它答对的概率就会显著增加。
* **自我反思**：模型会生成像 *"Wait, let me double check that calculation..."* 或者 *"No, that approach is wrong, let me try another way..."* 这样的文本。
* 这不是模仿人类，这是 **优化过程中的涌现行为（Emergent Behavior）**。它学会了“利用测试时间计算（Test-time Compute）”。

**[02:37:14]** OpenAI 的 **o1** 和 **o3** 系列模型也是基于同样的原理。它们被称为 **推理模型（Reasoning Models）** 或 **思考模型（Thinking Models）**。
* 它们在回答之前会花费大量时间（生成大量不可见的思维 Token）。
* 对于复杂任务（数学、代码、逻辑谜题），它们表现极佳。
* 对于简单任务（问候、事实查询），它们可能显得反应迟钝且过度思考。

---

## 12. AlphaGo 的 "Move 37" 时刻

**[02:45:32]** RL 的核心承诺是：**它不受人类能力的限制（Beyond Human Data）**。

* **SFT 的天花板**：如果你只是模仿人类（SFT），你永远无法超越人类专家的水平。你的上限就是你数据的质量。
* **RL 的无限可能**：AlphaGo 在与李世石的对局中下出了著名的 **"Move 37"（第 37 手）**。
    * 这一手棋在当时所有人类专家看来都是错的，甚至被认为是“滑标”。
    * 但事实证明，这是极其深远的一步，直接奠定了胜局。
    * 这是 AlphaGo 通过自我对弈（RL）发现的、**超越了人类围棋理论** 的新策略。

**[02:48:00]** 在 LLM 领域，我们也期望通过 RL 让模型发现人类未知的推理策略、代码优化方法或者科学发现。我们不再通过模仿人类来教 AI，而是通过设定目标让 AI 自我进化。

---

## 13. 基于人类反馈的强化学习 (RLHF)

**[02:51:19]** 但是，并非所有任务都有客观的“正确答案”（如数学题）。
* 任务：写一首关于秋天的诗。
* 任务：讲一个好笑的笑话。
* 任务：总结这篇新闻。

这些任务没有标准答案（Ground Truth）。我们如何对它们进行 RL？
这就是 **基于人类反馈的强化学习（RLHF）** 的用武之地。

### 13.1 奖励模型 (Reward Model)

**[02:53:00]** 过程如下：
1.  让模型对同一个提示生成多个不同的回答（A, B, C, D）。
2.  让人类标注员对这些回答进行 **排名（Ranking）**：A > C > B > D。
3.  训练一个独立的神经网络，称为 **奖励模型（Reward Model）**。
    * 输入：提示 + 回答。
    * 输出：一个分数（Scalar Score）。
    * 目标：预测人类的排名偏好。

### 13.2 针对奖励模型优化

**[02:56:00]** 一旦有了奖励模型，它就变成了我们的“代理老师”。
* 我们不再需要人类实时打分。
* 我们让 LLM 生成文本，奖励模型打分，然后通过 RL 算法（如 PPO）优化 LLM，使其生成能获得高分的文本。

**[03:00:49]** **RLHF 的局限性**：
* **古德哈特定律 (Goodhart's Law)**：当一个指标成为目标时，它就不再是一个好的指标。
* **博弈 (Gaming)**：如果 RL 运行太久，模型会找到欺骗奖励模型的方法。它会生成一些人类看不懂但奖励模型给高分的奇怪文本（Adversarial Examples）。
* 因此，RLHF 通常只进行 **少量的微调**（KL Divergence 约束），而不像数学领域的纯 RL 那样可以进行无限的自我提升。

---

## 14. 总结与未来展望

**[03:07:01]** **总结：构建 ChatGPT 的配方**

1.  **预训练 (Pre-training)**：
    * **数据**：互联网文本 (FineWeb)。
    * **算法**：Transformer, Next Token Prediction。
    * **产物**：基座模型（Base Model）。
    * **本质**：互联网模拟器，拥有知识但只会补全。

2.  **监督微调 (SFT)**：
    * **数据**：人类对话示例。
    * **产物**：SFT 模型（Assistant）。
    * **本质**：人类标注员的模拟器，学会了对话格式。

3.  **强化学习 (RL)**：
    * **方法**：Pure RL (数学/代码) 或 RLHF (创意/通用)。
    * **产物**：RL 模型（Reasoning Model）。
    * **本质**：通过试错发现更优思维路径，甚至超越人类。

### 14.1 瑞士奶酪模型 (Swiss Cheese Model)

**[03:08:53]** 对待 LLM 的正确态度：
* 它们的能力像 **瑞士奶酪**。大部分地方很强（知识渊博、逻辑通顺），但会有随机的孔洞（幻觉、计算错误、拼写盲区）。
* 你不知道孔洞在哪里。对于某些问题它表现得像天才，换个问法它可能就像个傻瓜。
* **永远不要完全信任**：把它当作一个得力的助手或草稿生成器，但 **必须** 对结果进行核实（Verify）。

### 14.2 未来趋势

**[03:09:43]** 接下来会发生什么？
1.  **多模态 (Multimodality)**：音频、视频、图像本质上都可以被 Token 化。未来的模型将原生支持所有这些模态，不再需要分开处理。
2.  **智能体 (Agents)**：模型不仅能说话，还能操作计算机、浏览网页、使用软件，执行长时间运行的任务。
3.  **测试时训练 (Test-time Training)**：像 o1/R1 这样的模型，通过在推理阶段消耗更多算力来换取更高的智能。这将是短期内的一个大趋势。

**[03:09:50]** 感谢观看。希望这个心智模型能帮助你更好地理解和使用这些强大的工具。

---

## 关键术语表 (完整版)

| 英文 | 中文 | 说明 |
|------|------|------|
| **Large Language Model (LLM)** | 大型语言模型 | 能够理解和生成人类语言的庞大神经网络模型。 |
| **Pre-training** | 预训练 | 使用海量互联网数据训练模型的初始阶段，使其获得广泛知识。 |
| **Tokenization / Token** | 词元化 / 词元 | 将文本分解为模型可处理的最小单位（如单词片段）的过程。 |
| **Transformer** | Transformer | 现代 LLM 所基于的神经网络架构，擅长处理序列数据。 |
| **Next Token Prediction** | 下一个词预测 | LLM 的核心训练任务，根据前面的内容预测下一个词。 |
| **Base Model** | 基座模型 | 仅经过预训练的模型，通常表现为文档补全而非对话助手。 |
| **Post-training** | 后训练 | 在预训练之后，将模型转化为有用助手的阶段 (SFT + RL)。 |
| **Supervised Fine-Tuning (SFT)** | 监督微调 | 使用人类编写的对话示例训练模型，使其模仿助手行为。 |
| **Hallucination** | 幻觉 | 模型自信地生成错误或虚构信息的现象。 |
| **In-Context Learning** | 上下文学习 | 通过在提示词中提供示例，让模型在不更新参数的情况下学习新任务。 |
| **Reinforcement Learning (RL)** | 强化学习 | 通过试错和奖励机制优化模型行为的训练方法。 |
| **Chain of Thought (CoT)** | 思维链 | 模型在给出最终答案前生成的一系列中间推理步骤。 |
| **RLHF** | 基于人类反馈的强化学习 | 使用人类偏好数据训练奖励模型，再用 RL 优化生成模型的技术。 |
| **Reward Model** | 奖励模型 | 模仿人类偏好给模型生成结果打分的神经网络。 |
| **Context Window** | 上下文窗口 | 模型在一次交互中能够“看到”和处理的 Token 最大数量（工作记忆）。 |
| **Weights / Parameters** | 权重 / 参数 | 神经网络内部存储知识的数值，训练过程中会被不断调整。 |